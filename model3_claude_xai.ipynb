{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpv3sxRVl5U3O6lqdEE7pn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/doctorat/blob/main/model3_claude_xai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRlBLN4FY4ZG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, GlobalMaxPooling1D, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import optuna\n",
        "import shap\n",
        "import lime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class StudentDropoutPredictor:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.event_types = ['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']\n",
        "        self.n_weeks = 4\n",
        "        self.n_events = 7\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and parse the CSV files\"\"\"\n",
        "        print(\"Loading data files...\")\n",
        "\n",
        "        # Load truth data (enrollment_id, dropout_label)\n",
        "        truth_df = pd.read_csv('truth_train.csv', header=None, names=['enrollment_id', 'dropout'])\n",
        "\n",
        "        # Load enrollment data\n",
        "        enrollment_df = pd.read_csv('enrollment_train.csv')\n",
        "\n",
        "        # Load log data\n",
        "        log_df = pd.read_csv('log_train spliting.csv')\n",
        "        log_df['time'] = pd.to_datetime(log_df['time'])\n",
        "\n",
        "        print(f\"Truth data shape: {truth_df.shape}\")\n",
        "        print(f\"Enrollment data shape: {enrollment_df.shape}\")\n",
        "        print(f\"Log data shape: {log_df.shape}\")\n",
        "\n",
        "        return truth_df, enrollment_df, log_df\n",
        "\n",
        "    def create_temporal_features(self, log_df, truth_df):\n",
        "        \"\"\"\n",
        "        Create Model (3) structure: (1 student, 4 weeks, 7 events)\n",
        "        Each student gets a 3D tensor of shape (4 weeks, 7 events)\n",
        "        \"\"\"\n",
        "        print(\"Creating temporal features...\")\n",
        "\n",
        "        # Get unique enrollment IDs from truth data\n",
        "        enrollment_ids = truth_df['enrollment_id'].unique()\n",
        "\n",
        "        # Initialize the feature matrix\n",
        "        n_students = len(enrollment_ids)\n",
        "        X = np.zeros((n_students, self.n_weeks, self.n_events))\n",
        "        y = np.zeros(n_students)\n",
        "        student_ids = []\n",
        "\n",
        "        # Create mapping from enrollment_id to dropout label\n",
        "        truth_map = dict(zip(truth_df['enrollment_id'], truth_df['dropout']))\n",
        "\n",
        "        for idx, enrollment_id in enumerate(enrollment_ids):\n",
        "            if enrollment_id not in truth_map:\n",
        "                continue\n",
        "\n",
        "            student_ids.append(enrollment_id)\n",
        "            y[idx] = truth_map[enrollment_id]\n",
        "\n",
        "            # Get student's log data\n",
        "            student_logs = log_df[log_df['enrollment_id'] == enrollment_id].copy()\n",
        "\n",
        "            if len(student_logs) == 0:\n",
        "                continue\n",
        "\n",
        "            # Sort by time\n",
        "            student_logs = student_logs.sort_values('time')\n",
        "\n",
        "            # Get the first activity time as reference\n",
        "            start_time = student_logs['time'].min()\n",
        "\n",
        "            # Create weekly bins\n",
        "            for _, row in student_logs.iterrows():\n",
        "                # Calculate which week this activity belongs to\n",
        "                time_diff = (row['time'] - start_time).days\n",
        "                week_idx = min(time_diff // 7, self.n_weeks - 1)  # Cap at 4 weeks\n",
        "\n",
        "                # Find event index\n",
        "                if row['event'] in self.event_types:\n",
        "                    event_idx = self.event_types.index(row['event'])\n",
        "                    X[idx, week_idx, event_idx] += 1  # Count occurrences\n",
        "\n",
        "        # Filter out students with no data\n",
        "        valid_indices = [i for i, sid in enumerate(student_ids) if sid in truth_map]\n",
        "        X = X[valid_indices]\n",
        "        y = y[valid_indices]\n",
        "        student_ids = [student_ids[i] for i in valid_indices]\n",
        "\n",
        "        print(f\"Created features for {len(student_ids)} students\")\n",
        "        print(f\"Feature matrix shape: {X.shape}\")\n",
        "        print(f\"Dropout distribution: {np.bincount(y.astype(int))}\")\n",
        "\n",
        "        return X, y, student_ids\n",
        "\n",
        "    def create_cnn_lstm_model(self, trial=None):\n",
        "        \"\"\"\n",
        "        Create CNN+LSTM model for dropout prediction\n",
        "        Model (3): (1 student, 4 weeks, 7 events)\n",
        "        \"\"\"\n",
        "        if trial:\n",
        "            # Hyperparameter tuning mode\n",
        "            lstm_units = trial.suggest_int('lstm_units', 32, 128)\n",
        "            conv_filters = trial.suggest_int('conv_filters', 16, 64)\n",
        "            conv_kernel = trial.suggest_int('conv_kernel', 2, 4)\n",
        "            dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
        "            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "        else:\n",
        "            # Default parameters\n",
        "            lstm_units = 64\n",
        "            conv_filters = 32\n",
        "            conv_kernel = 3\n",
        "            dropout_rate = 0.3\n",
        "            learning_rate = 0.001\n",
        "\n",
        "        # Input layer: (batch_size, 4 weeks, 7 events)\n",
        "        input_layer = Input(shape=(self.n_weeks, self.n_events), name='student_activity')\n",
        "\n",
        "        # CNN layers to capture local patterns\n",
        "        conv1 = Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n",
        "                      activation='relu', padding='same')(input_layer)\n",
        "        conv2 = Conv1D(filters=conv_filters*2, kernel_size=conv_kernel,\n",
        "                      activation='relu', padding='same')(conv1)\n",
        "\n",
        "        # LSTM layers to capture temporal dependencies\n",
        "        lstm1 = LSTM(lstm_units, return_sequences=True, dropout=dropout_rate)(conv2)\n",
        "        lstm2 = LSTM(lstm_units//2, return_sequences=False, dropout=dropout_rate)(lstm1)\n",
        "\n",
        "        # Global max pooling for CNN features\n",
        "        global_pool = GlobalMaxPooling1D()(conv2)\n",
        "\n",
        "        # Combine LSTM and CNN features\n",
        "        combined = concatenate([lstm2, global_pool])\n",
        "\n",
        "        # Dense layers for classification\n",
        "        dense1 = Dense(64, activation='relu')(combined)\n",
        "        dropout1 = Dropout(dropout_rate)(dense1)\n",
        "        dense2 = Dense(32, activation='relu')(dropout1)\n",
        "        dropout2 = Dropout(dropout_rate)(dense2)\n",
        "\n",
        "        # Output layer\n",
        "        output = Dense(1, activation='sigmoid', name='dropout_prediction')(dropout2)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train, y_train, X_val, y_val, n_trials=50):\n",
        "        \"\"\"Perform hyperparameter tuning using Optuna\"\"\"\n",
        "        print(f\"Starting hyperparameter tuning with {n_trials} trials...\")\n",
        "\n",
        "        def objective(trial):\n",
        "            # Create model with trial parameters\n",
        "            model = self.create_cnn_lstm_model(trial)\n",
        "\n",
        "            # Early stopping\n",
        "            early_stopping = EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            # Train model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Return validation AUC as objective\n",
        "            val_pred = model.predict(X_val, verbose=0)\n",
        "            val_auc = roc_auc_score(y_val, val_pred)\n",
        "\n",
        "            return val_auc\n",
        "\n",
        "        # Create study\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "        print(f\"Best trial: {study.best_trial.number}\")\n",
        "        print(f\"Best AUC: {study.best_value:.4f}\")\n",
        "        print(f\"Best parameters: {study.best_params}\")\n",
        "\n",
        "        return study.best_params\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val, best_params=None):\n",
        "        \"\"\"Train the CNN+LSTM model\"\"\"\n",
        "        print(\"Training the model...\")\n",
        "\n",
        "        # Create model with best parameters\n",
        "        if best_params:\n",
        "            # Create a mock trial with best parameters\n",
        "            class MockTrial:\n",
        "                def __init__(self, params):\n",
        "                    self.params = params\n",
        "                def suggest_int(self, name, *args):\n",
        "                    return self.params[name]\n",
        "                def suggest_float(self, name, *args):\n",
        "                    return self.params[name]\n",
        "\n",
        "            mock_trial = MockTrial(best_params)\n",
        "            self.model = self.create_cnn_lstm_model(mock_trial)\n",
        "        else:\n",
        "            self.model = self.create_cnn_lstm_model()\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Handle class imbalance\n",
        "        class_weights = {\n",
        "            0: len(y_train) / (2 * np.sum(y_train == 0)),\n",
        "            1: len(y_train) / (2 * np.sum(y_train == 1))\n",
        "        }\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "        \"\"\"Evaluate the model and return metrics\"\"\"\n",
        "        print(\"Evaluating model...\")\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_prob = self.model.predict(X_test)\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "        # Metrics\n",
        "        auc_score = roc_auc_score(y_test, y_pred_prob)\n",
        "        print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(cm)\n",
        "\n",
        "        return {\n",
        "            'auc': auc_score,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_pred_prob,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def explain_predictions_shap(self, X_sample, sample_size=100):\n",
        "        \"\"\"Generate SHAP explanations for model predictions\"\"\"\n",
        "        print(\"Generating SHAP explanations...\")\n",
        "\n",
        "        # Sample data for SHAP (to reduce computation time)\n",
        "        if len(X_sample) > sample_size:\n",
        "            indices = np.random.choice(len(X_sample), sample_size, replace=False)\n",
        "            X_shap = X_sample[indices]\n",
        "        else:\n",
        "            X_shap = X_sample\n",
        "\n",
        "        # Create SHAP explainer\n",
        "        explainer = shap.DeepExplainer(self.model, X_shap[:50])  # Use subset as background\n",
        "        shap_values = explainer.shap_values(X_shap[:20])  # Explain subset\n",
        "\n",
        "        return explainer, shap_values, X_shap[:20]\n",
        "\n",
        "    def plot_feature_importance(self, shap_values, X_sample):\n",
        "        \"\"\"Plot feature importance using SHAP values\"\"\"\n",
        "        print(\"Plotting feature importance...\")\n",
        "\n",
        "        # Reshape SHAP values and features for plotting\n",
        "        shap_vals_reshaped = shap_values[0].reshape(-1, self.n_weeks * self.n_events)\n",
        "        X_reshaped = X_sample.reshape(-1, self.n_weeks * self.n_events)\n",
        "\n",
        "        # Create feature names\n",
        "        feature_names = []\n",
        "        for week in range(self.n_weeks):\n",
        "            for event in self.event_types:\n",
        "                feature_names.append(f\"Week{week+1}_{event}\")\n",
        "\n",
        "        # Summary plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(shap_vals_reshaped, X_reshaped,\n",
        "                         feature_names=feature_names, show=False)\n",
        "        plt.title(\"SHAP Feature Importance for Dropout Prediction\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return feature_names\n",
        "\n",
        "    def create_temporal_heatmap(self, X_sample, predictions, n_samples=5):\n",
        "        \"\"\"Create heatmaps showing temporal patterns for sample students\"\"\"\n",
        "        print(\"Creating temporal activity heatmaps...\")\n",
        "\n",
        "        fig, axes = plt.subplots(n_samples, 1, figsize=(12, 3*n_samples))\n",
        "        if n_samples == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i in range(min(n_samples, len(X_sample))):\n",
        "            # Student activity matrix\n",
        "            student_data = X_sample[i]\n",
        "            dropout_prob = predictions[i][0]\n",
        "\n",
        "            # Create heatmap\n",
        "            sns.heatmap(student_data.T,\n",
        "                       xticklabels=[f'Week {j+1}' for j in range(self.n_weeks)],\n",
        "                       yticklabels=self.event_types,\n",
        "                       cmap='YlOrRd',\n",
        "                       annot=True,\n",
        "                       fmt='.0f',\n",
        "                       ax=axes[i])\n",
        "\n",
        "            axes[i].set_title(f'Student {i+1} - Dropout Probability: {dropout_prob:.3f}')\n",
        "            axes[i].set_xlabel('Time Period')\n",
        "            axes[i].set_ylabel('Event Type')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Initialize predictor\n",
        "    predictor = StudentDropoutPredictor()\n",
        "\n",
        "    # Load data\n",
        "    truth_df, enrollment_df, log_df = predictor.load_data()\n",
        "\n",
        "    # Create temporal features (Model 3 structure)\n",
        "    X, y, student_ids = predictor.create_temporal_features(log_df, truth_df)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42, stratify=y\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    best_params = predictor.hyperparameter_tuning(X_train, y_train, X_val, y_val, n_trials=20)\n",
        "\n",
        "    # Train model with best parameters\n",
        "    history = predictor.train_model(X_train, y_train, X_val, y_val, best_params)\n",
        "\n",
        "    # Evaluate model\n",
        "    results = predictor.evaluate_model(X_test, y_test)\n",
        "\n",
        "    # XAI: SHAP explanations\n",
        "    explainer, shap_values, X_shap_sample = predictor.explain_predictions_shap(X_test)\n",
        "\n",
        "    # Plot feature importance\n",
        "    feature_names = predictor.plot_feature_importance(shap_values, X_shap_sample)\n",
        "\n",
        "    # Create temporal heatmaps\n",
        "    predictor.create_temporal_heatmap(X_test[:5], results['probabilities'][:5])\n",
        "\n",
        "    print(\"\\n=== Model Training and Evaluation Complete ===\")\n",
        "    print(f\"Final AUC Score: {results['auc']:.4f}\")\n",
        "    print(f\"Best hyperparameters: {best_params}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}