{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF2JdWUGcwIwmm8LFg3zW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/doctorat/blob/main/xai_deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3RZk2rWgD7g",
        "outputId": "7f090f33-a19c-4d6f-cb28-ba01f69e28ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsKhbuO_e7Im",
        "outputId": "51172582-6ce1-4fee-eea0-1b7ac02b5f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution:\n",
            "dropout\n",
            "1    0.792927\n",
            "0    0.207073\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "==================================================\n",
            "Training and tuning RandomForest\n",
            "==================================================\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "Best parameters found:\n",
            "{'model__max_depth': 20, 'model__max_features': 'sqrt', 'model__min_samples_split': 5, 'model__n_estimators': 200}\n",
            "Best accuracy: 0.8545\n",
            "\n",
            "RandomForest Performance:\n",
            "Accuracy: 0.8498900825417893\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.61      0.63      4992\n",
            "           1       0.90      0.91      0.91     19117\n",
            "\n",
            "    accuracy                           0.85     24109\n",
            "   macro avg       0.77      0.76      0.77     24109\n",
            "weighted avg       0.85      0.85      0.85     24109\n",
            "\n",
            "ROC AUC: 0.8438\n",
            "Average Precision: 0.9371\n",
            "Confusion Matrix:\n",
            "[[ 3059  1933]\n",
            " [ 1686 17431]]\n",
            "\n",
            "==================================================\n",
            "Training and tuning XGBoost\n",
            "==================================================\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                            roc_auc_score, roc_curve, precision_recall_curve,\n",
        "                            average_precision_score, f1_score, make_scorer)\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import shap\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess(file_path):\n",
        "    \"\"\"Load the dataset and perform initial preprocessing\"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Drop identifier columns\n",
        "    data = data.drop(['username', 'enrollment_id', 'course_id'], axis=1)\n",
        "\n",
        "    # Check class distribution\n",
        "    print(\"Class distribution:\")\n",
        "    print(data['dropout'].value_counts(normalize=True))\n",
        "\n",
        "    return data\n",
        "\n",
        "# Feature engineering\n",
        "def create_features(data):\n",
        "    \"\"\"Create additional features from the raw data\"\"\"\n",
        "    X = data.drop('dropout', axis=1)\n",
        "    y = data['dropout']\n",
        "\n",
        "    activities = ['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']\n",
        "\n",
        "    # Weekly aggregates\n",
        "    for activity in activities:\n",
        "        for week in range(4):\n",
        "            start_day = week * 7 + 1\n",
        "            end_day = (week + 1) * 7\n",
        "            cols = [f'day_{d}_{activity}' for d in range(start_day, end_day+1)]\n",
        "            X[f'week_{week+1}_{activity}_sum'] = X[cols].sum(axis=1)\n",
        "            X[f'week_{week+1}_{activity}_mean'] = X[cols].mean(axis=1)\n",
        "\n",
        "        # Total activity features\n",
        "        cols = [f'day_{d}_{activity}' for d in range(1, 31)]\n",
        "        X[f'total_{activity}_sum'] = X[cols].sum(axis=1)\n",
        "        X[f'total_{activity}_mean'] = X[cols].mean(axis=1)\n",
        "\n",
        "    # Activity duration features\n",
        "    for activity in activities:\n",
        "        activity_cols = [f'day_{d}_{activity}' for d in range(1, 31)]\n",
        "        # Last day with activity\n",
        "        last_active = X[activity_cols].gt(0).idxmax(axis=1)\n",
        "        X[f'last_active_day_{activity}'] = last_active.str.extract('(\\d+)').astype(float)\n",
        "        # Days since last activity\n",
        "        X[f'days_since_{activity}'] = 30 - X[f'last_active_day_{activity}']\n",
        "\n",
        "    # Engagement pattern features\n",
        "    X['total_engagement'] = X[[f'total_{act}_sum' for act in activities]].sum(axis=1)\n",
        "    X['engagement_variance'] = X[[f'total_{act}_sum' for act in activities]].var(axis=1)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Model evaluation\n",
        "def evaluate_model(model, X_test, y_test, model_name=\"\"):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "    print(f\"Average Precision: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'roc_auc': roc_auc_score(y_test, y_proba),\n",
        "        'f1': f1_score(y_test, y_pred),\n",
        "        'precision': average_precision_score(y_test, y_proba)\n",
        "    }\n",
        "\n",
        "# Hyperparameter tuning with cross-validation\n",
        "def tune_model(X, y, model, param_grid, scoring='accuracy'):\n",
        "    \"\"\"Perform hyperparameter tuning with cross-validation\"\"\"\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Create pipeline with SMOTE and scaler\n",
        "    pipeline = imbpipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('sampling', SMOTE(random_state=42)),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid={'model__' + key: value for key, value in param_grid.items()},\n",
        "        scoring=scoring,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    print(\"\\nBest parameters found:\")\n",
        "    print(grid_search.best_params_)\n",
        "    print(f\"Best {scoring}: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    data = load_and_preprocess('model1_210_features.csv')\n",
        "\n",
        "    # Feature engineering\n",
        "    X, y = create_features(data)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Define models and parameter grids for tuning\n",
        "    models = {\n",
        "        'RandomForest': {\n",
        "            'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': [100, 200],\n",
        "                'max_depth': [None, 10, 20],\n",
        "                'min_samples_split': [2, 5],\n",
        "                'max_features': ['sqrt', 'log2']\n",
        "            }\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "            'params': {\n",
        "                'learning_rate': [0.01, 0.1],\n",
        "                'max_depth': [3, 5, 7],\n",
        "                'n_estimators': [100, 200],\n",
        "                'subsample': [0.8, 1.0],\n",
        "                'gamma': [0, 0.1]\n",
        "            }\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'model': LGBMClassifier(random_state=42),\n",
        "            'params': {\n",
        "                'learning_rate': [0.01, 0.1],\n",
        "                'num_leaves': [31, 63],\n",
        "                'max_depth': [-1, 5, 10],\n",
        "                'n_estimators': [100, 200],\n",
        "                'min_child_samples': [20, 50]\n",
        "            }\n",
        "        },\n",
        "        'CatBoost': {\n",
        "            'model': CatBoostClassifier(random_state=42, verbose=0),\n",
        "            'params': {\n",
        "                'iterations': [100, 200],\n",
        "                'depth': [4, 6, 8],\n",
        "                'learning_rate': [0.01, 0.1],\n",
        "                'l2_leaf_reg': [1, 3]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Scale positive weight for boosting algorithms\n",
        "    scale_pos_weight = len(y_train[y_train==0])/len(y_train[y_train==1])\n",
        "\n",
        "    # Train and evaluate models\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    results = {}\n",
        "\n",
        "    for name, config in models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training and tuning {name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Adjust for class imbalance in boosting algorithms\n",
        "        if name in ['XGBoost', 'LightGBM']:\n",
        "            config['model'].set_params(scale_pos_weight=scale_pos_weight)\n",
        "\n",
        "        # Tune model\n",
        "        model = tune_model(X_train, y_train, config['model'], config['params'], scoring='accuracy')\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_results = evaluate_model(model, X_test, y_test, name)\n",
        "        results[name] = test_results\n",
        "\n",
        "        # Track best model\n",
        "        if test_results['accuracy'] > best_accuracy:\n",
        "            best_accuracy = test_results['accuracy']\n",
        "            best_model = model\n",
        "\n",
        "    # Print summary of results\n",
        "    print(\"\\n\\nModel Comparison:\")\n",
        "    for name, res in results.items():\n",
        "        print(f\"{name}: Accuracy = {res['accuracy']:.4f}, ROC AUC = {res['roc_auc']:.4f}\")\n",
        "\n",
        "    # Explain best model\n",
        "    print(f\"\\nBest model: {type(best_model.named_steps['model']).__name__}\")\n",
        "    print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    # SHAP explanation\n",
        "    print(\"\\nGenerating SHAP explanations...\")\n",
        "    explainer = shap.TreeExplainer(best_model.named_steps['model'])\n",
        "    X_train_scaled = best_model.named_steps['scaler'].transform(X_train)\n",
        "    shap_values = explainer.shap_values(X_train_scaled)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_train_scaled, feature_names=X.columns, plot_type=\"bar\")\n",
        "    plt.title(\"Feature Importance (SHAP)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance\n",
        "    if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
        "        feature_importance = best_model.named_steps['model'].feature_importances_\n",
        "        sorted_idx = np.argsort(feature_importance)\n",
        "\n",
        "        plt.figure(figsize=(10, 12))\n",
        "        plt.barh(range(len(sorted_idx[-20:])), feature_importance[sorted_idx[-20:]])\n",
        "        plt.yticks(range(len(sorted_idx[-20:])), np.array(X.columns)[sorted_idx[-20:]])\n",
        "        plt.xlabel(\"Feature Importance\")\n",
        "        plt.title(\"Top 20 Feature Importances\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Partial dependence plots\n",
        "    top_features = np.array(X.columns)[sorted_idx[-5:]]\n",
        "    print(\"\\nPartial Dependence Plots for Top Features:\", top_features)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    PartialDependenceDisplay.from_estimator(\n",
        "        best_model,\n",
        "        X_train,\n",
        "        features=top_features,\n",
        "        feature_names=X.columns,\n",
        "        grid_resolution=20\n",
        "    )\n",
        "    plt.suptitle(\"Partial Dependence Plots\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Threshold optimization\n",
        "    print(\"\\nOptimizing decision threshold...\")\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    thresholds = np.linspace(0.1, 0.9, 50)\n",
        "    accuracies = []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        y_pred_thresh = (y_proba >= thresh).astype(int)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred_thresh))\n",
        "\n",
        "    best_thresh = thresholds[np.argmax(accuracies)]\n",
        "    print(f\"Best threshold: {best_thresh:.2f} with accuracy: {max(accuracies):.4f}\")\n",
        "\n",
        "    # Evaluate with optimal threshold\n",
        "    y_pred_opt = (y_proba >= best_thresh).astype(int)\n",
        "    print(\"\\nPerformance with optimized threshold:\")\n",
        "    print(classification_report(y_test, y_pred_opt))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_opt))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}