{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiBoICt1autayn4FD0wTp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/doctorat/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwOAeXUmqVqY",
        "outputId": "8df0a6fe-dcbc-4aed-ff4a-fef2ecfb669b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Enhanced packages required:\n",
            "   - tensorflow>=2.10.0\n",
            "   - pandas>=1.3.0\n",
            "   - numpy>=1.21.0\n",
            "   - scikit-learn>=1.0.0\n",
            "   - matplotlib>=3.3.0\n",
            "   - seaborn>=0.11.0\n",
            "\n",
            "Install with: pip install tensorflow pandas numpy scikit-learn matplotlib seaborn\n",
            "\n",
            "🎓 ADVANCED Student Dropout Prediction with Ensemble VAE + LSTM\n",
            "======================================================================\n",
            "📂 Loading and preprocessing data with advanced techniques...\n",
            "Dataset shape: (120542, 214)\n",
            "🔧 Engineering advanced features...\n",
            "Created 74 engineered features\n",
            "Enhanced features created: 74 additional features\n",
            "Class distribution: Non-dropout: 24961 (20.71%)\n",
            "                   Dropout: 95581 (79.29%)\n",
            "✅ Advanced preprocessing completed!\n",
            "🧠 Building Advanced Conditional VAE...\n",
            "✅ Advanced VAE built with skip connections and enhanced loss!\n",
            "🏋️ Training Advanced VAE...\n",
            "Training VAE on 24961 minority samples\n",
            "Epoch 1/80\n",
            "❌ Error during execution: Dimensions must be equal, but are 64 and 210 for '{{node compile_loss/advanced_vae_loss/mul}} = Mul[T=DT_FLOAT](compile_loss/advanced_vae_loss/Mean, compile_loss/advanced_vae_loss/ones_like)' with input shapes: [64], [64,210].\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, Dict, Any, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class AdvancedStudentDropoutPredictor:\n",
        "    \"\"\"\n",
        "    Advanced implementation with optimized features for maximum accuracy:\n",
        "\n",
        "    1. Enhanced feature engineering (temporal trends, ratios, aggregations)\n",
        "    2. Multiple model architectures (CNN-LSTM, Transformer-like attention)\n",
        "    3. Advanced VAE with conditional generation\n",
        "    4. Ensemble methods and cross-validation\n",
        "    5. Sophisticated preprocessing for sparse data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = RobustScaler()  # Better for sparse data\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.models = {}\n",
        "        self.original_data = None\n",
        "        self.enhanced_data = None\n",
        "        self.feature_columns = []\n",
        "        self.engineered_features = []\n",
        "\n",
        "    def load_and_preprocess_data(self, filepath: str) -> None:\n",
        "        \"\"\"Enhanced data loading with advanced preprocessing.\"\"\"\n",
        "        print(\"📂 Loading and preprocessing data with advanced techniques...\")\n",
        "\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "        # Extract base feature columns\n",
        "        self.feature_columns = []\n",
        "        for day in range(1, 31):\n",
        "            for activity in ['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']:\n",
        "                self.feature_columns.append(f'day_{day}_{activity}')\n",
        "\n",
        "        X = df[self.feature_columns].values\n",
        "        y = df['dropout'].values\n",
        "\n",
        "        # Handle missing values with domain knowledge\n",
        "        X = np.nan_to_num(X, nan=0.0)\n",
        "\n",
        "        # Create enhanced features\n",
        "        X_enhanced = self._create_enhanced_features(X)\n",
        "\n",
        "        # Advanced normalization\n",
        "        X_normalized = self.scaler.fit_transform(X)\n",
        "        X_enhanced_normalized = self.feature_scaler.fit_transform(X_enhanced)\n",
        "\n",
        "        # Multiple data representations\n",
        "        X_lstm = X_normalized.reshape(X_normalized.shape[0], 30, 7)  # For LSTM\n",
        "        X_cnn = X_normalized.reshape(X_normalized.shape[0], 30, 7, 1)  # For CNN\n",
        "\n",
        "        # Class analysis\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        class_dist = dict(zip(unique, counts))\n",
        "\n",
        "        print(f\"Enhanced features created: {X_enhanced.shape[1]} additional features\")\n",
        "        print(f\"Class distribution: Non-dropout: {class_dist.get(0, 0)} ({class_dist.get(0, 0)/len(y)*100:.2f}%)\")\n",
        "        print(f\"                   Dropout: {class_dist.get(1, 0)} ({class_dist.get(1, 0)/len(y)*100:.2f}%)\")\n",
        "\n",
        "        self.original_data = {\n",
        "            'X_flat': X_normalized,\n",
        "            'X_lstm': X_lstm,\n",
        "            'X_cnn': X_cnn,\n",
        "            'X_enhanced': X_enhanced_normalized,\n",
        "            'y': y,\n",
        "            'class_distribution': class_dist\n",
        "        }\n",
        "\n",
        "        print(\"✅ Advanced preprocessing completed!\")\n",
        "\n",
        "    def _create_enhanced_features(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Create advanced engineered features based on domain insights.\"\"\"\n",
        "        print(\"🔧 Engineering advanced features...\")\n",
        "\n",
        "        # Reshape to (samples, days, activities)\n",
        "        X_reshaped = X.reshape(X.shape[0], 30, 7)\n",
        "\n",
        "        features = []\n",
        "        feature_names = []\n",
        "\n",
        "        # 1. Temporal aggregation features\n",
        "        # Weekly aggregations\n",
        "        for week in range(4):\n",
        "            start_day = week * 7\n",
        "            end_day = min((week + 1) * 7, 30)\n",
        "            week_data = X_reshaped[:, start_day:end_day, :].sum(axis=1)\n",
        "\n",
        "            for activity_idx, activity in enumerate(['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']):\n",
        "                features.append(week_data[:, activity_idx])\n",
        "                feature_names.append(f'week_{week+1}_{activity}')\n",
        "\n",
        "        # 2. Temporal trend features\n",
        "        # Early vs Late engagement\n",
        "        early_engagement = X_reshaped[:, :7, :].sum(axis=(1, 2))  # First week\n",
        "        late_engagement = X_reshaped[:, -7:, :].sum(axis=(1, 2))  # Last week\n",
        "\n",
        "        features.extend([\n",
        "            early_engagement,\n",
        "            late_engagement,\n",
        "            np.log1p(early_engagement / (late_engagement + 1e-8)),  # Log ratio\n",
        "            early_engagement - late_engagement  # Difference\n",
        "        ])\n",
        "        feature_names.extend(['early_engagement', 'late_engagement', 'engagement_ratio', 'engagement_decline'])\n",
        "\n",
        "        # 3. Activity-specific features\n",
        "        activity_names = ['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']\n",
        "        for i, activity in enumerate(activity_names):\n",
        "            activity_data = X_reshaped[:, :, i]\n",
        "\n",
        "            # Total activity\n",
        "            total_activity = activity_data.sum(axis=1)\n",
        "\n",
        "            # Days active (non-zero days)\n",
        "            days_active = (activity_data > 0).sum(axis=1)\n",
        "\n",
        "            # Peak activity day\n",
        "            peak_day = np.argmax(activity_data, axis=1)\n",
        "\n",
        "            # Activity consistency (std/mean)\n",
        "            activity_std = np.std(activity_data, axis=1)\n",
        "            activity_mean = np.mean(activity_data, axis=1)\n",
        "            consistency = activity_std / (activity_mean + 1e-8)\n",
        "\n",
        "            # Streak features (longest consecutive active days)\n",
        "            streaks = []\n",
        "            for sample_idx in range(activity_data.shape[0]):\n",
        "                sample_data = activity_data[sample_idx]\n",
        "                current_streak = 0\n",
        "                max_streak = 0\n",
        "                for day_val in sample_data:\n",
        "                    if day_val > 0:\n",
        "                        current_streak += 1\n",
        "                        max_streak = max(max_streak, current_streak)\n",
        "                    else:\n",
        "                        current_streak = 0\n",
        "                streaks.append(max_streak)\n",
        "\n",
        "            features.extend([\n",
        "                total_activity,\n",
        "                days_active,\n",
        "                peak_day,\n",
        "                consistency,\n",
        "                np.array(streaks)\n",
        "            ])\n",
        "            feature_names.extend([\n",
        "                f'{activity}_total',\n",
        "                f'{activity}_days_active',\n",
        "                f'{activity}_peak_day',\n",
        "                f'{activity}_consistency',\n",
        "                f'{activity}_max_streak'\n",
        "            ])\n",
        "\n",
        "        # 4. Cross-activity features\n",
        "        # High-value activities (problem, discussion, access) vs low-value\n",
        "        high_value_activities = X_reshaped[:, :, [0, 1, 3]].sum(axis=(1, 2))  # access, problem, discussion\n",
        "        low_value_activities = X_reshaped[:, :, [2, 4, 5, 6]].sum(axis=(1, 2))  # wiki, navigate, page_close, video\n",
        "\n",
        "        features.extend([\n",
        "            high_value_activities,\n",
        "            low_value_activities,\n",
        "            high_value_activities / (low_value_activities + 1e-8)\n",
        "        ])\n",
        "        feature_names.extend(['high_value_total', 'low_value_total', 'high_low_ratio'])\n",
        "\n",
        "        # 5. Temporal patterns\n",
        "        # First day activity (crucial predictor)\n",
        "        first_day_total = X_reshaped[:, 0, :].sum(axis=1)\n",
        "\n",
        "        # Activity decay rate (slope of activity over time)\n",
        "        daily_totals = X_reshaped.sum(axis=2)  # Sum across activities for each day\n",
        "        decay_rates = []\n",
        "        for sample_idx in range(daily_totals.shape[0]):\n",
        "            sample_daily = daily_totals[sample_idx]\n",
        "            # Calculate linear regression slope\n",
        "            x_vals = np.arange(30)\n",
        "            if np.std(sample_daily) > 0:\n",
        "                correlation = np.corrcoef(x_vals, sample_daily)[0, 1]\n",
        "                decay_rates.append(correlation)\n",
        "            else:\n",
        "                decay_rates.append(0)\n",
        "\n",
        "        features.extend([\n",
        "            first_day_total,\n",
        "            np.array(decay_rates)\n",
        "        ])\n",
        "        feature_names.extend(['first_day_total', 'activity_decay_rate'])\n",
        "\n",
        "        # 6. Sparsity-aware features\n",
        "        # Total non-zero entries\n",
        "        total_nonzero = (X_reshaped > 0).sum(axis=(1, 2))\n",
        "\n",
        "        # Activity diversity (how many different activity types used)\n",
        "        activity_diversity = (X_reshaped.sum(axis=1) > 0).sum(axis=1)\n",
        "\n",
        "        features.extend([\n",
        "            total_nonzero,\n",
        "            activity_diversity\n",
        "        ])\n",
        "        feature_names.extend(['total_nonzero_entries', 'activity_diversity'])\n",
        "\n",
        "        # Convert to array\n",
        "        feature_matrix = np.column_stack(features)\n",
        "\n",
        "        # Handle any remaining NaN or inf values\n",
        "        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        self.engineered_features = feature_names\n",
        "        print(f\"Created {feature_matrix.shape[1]} engineered features\")\n",
        "\n",
        "        return feature_matrix\n",
        "\n",
        "    def build_advanced_vae(self, input_dim: int, latent_dim: int = 64) -> None:\n",
        "        \"\"\"Build advanced conditional VAE with better architecture.\"\"\"\n",
        "        print(\"🧠 Building Advanced Conditional VAE...\")\n",
        "\n",
        "        # Encoder with skip connections and batch norm\n",
        "        encoder_inputs = keras.Input(shape=(input_dim,))\n",
        "\n",
        "        # Dense layers with skip connections\n",
        "        x1 = layers.Dense(256, activation='relu')(encoder_inputs)\n",
        "        x1 = layers.BatchNormalization()(x1)\n",
        "        x1 = layers.Dropout(0.3)(x1)\n",
        "\n",
        "        x2 = layers.Dense(128, activation='relu')(x1)\n",
        "        x2 = layers.BatchNormalization()(x2)\n",
        "        x2 = layers.Dropout(0.3)(x2)\n",
        "\n",
        "        # Skip connection\n",
        "        x_skip = layers.Dense(128, activation='relu')(encoder_inputs)\n",
        "        x2 = layers.Add()([x2, x_skip])\n",
        "\n",
        "        x3 = layers.Dense(64, activation='relu')(x2)\n",
        "        x3 = layers.BatchNormalization()(x3)\n",
        "\n",
        "        z_mean = layers.Dense(latent_dim, name='z_mean')(x3)\n",
        "        z_log_var = layers.Dense(latent_dim, name='z_log_var')(x3)\n",
        "\n",
        "        # Improved sampling with batch normalization\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "        self.encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "        d1 = layers.Dense(64, activation='relu')(latent_inputs)\n",
        "        d1 = layers.BatchNormalization()(d1)\n",
        "\n",
        "        d2 = layers.Dense(128, activation='relu')(d1)\n",
        "        d2 = layers.BatchNormalization()(d2)\n",
        "\n",
        "        # Skip connection in decoder\n",
        "        d_skip = layers.Dense(128, activation='relu')(latent_inputs)\n",
        "        d2 = layers.Add()([d2, d_skip])\n",
        "\n",
        "        d3 = layers.Dense(256, activation='relu')(d2)\n",
        "        d3 = layers.BatchNormalization()(d3)\n",
        "        d3 = layers.Dropout(0.2)(d3)\n",
        "\n",
        "        decoder_outputs = layers.Dense(input_dim, activation='sigmoid')(d3)\n",
        "\n",
        "        self.decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "        # VAE model with improved loss\n",
        "        outputs = self.decoder(z)\n",
        "        self.vae_model = Model(encoder_inputs, outputs, name='vae')\n",
        "\n",
        "        # Enhanced loss function with perceptual loss\n",
        "        def advanced_vae_loss(y_true, y_pred):\n",
        "            # Reconstruction loss with different weights for important features\n",
        "            reconstruction_loss = keras.losses.mse(y_true, y_pred)\n",
        "\n",
        "            # Weight important features more heavily\n",
        "            feature_weights = tf.ones_like(y_true)\n",
        "            # Give higher weight to high-discriminating activities (access, problem, discussion)\n",
        "            important_indices = []\n",
        "            for day in range(30):\n",
        "                important_indices.extend([day*7 + 0, day*7 + 1, day*7 + 3])  # access, problem, discussion\n",
        "\n",
        "            reconstruction_loss = tf.reduce_mean(reconstruction_loss * feature_weights)\n",
        "            reconstruction_loss *= input_dim\n",
        "\n",
        "            # KL divergence loss\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\n",
        "            kl_loss *= -0.5\n",
        "\n",
        "            # Sparsity regularization (encourage sparse reconstructions)\n",
        "            sparsity_loss = tf.reduce_mean(tf.abs(y_pred))\n",
        "\n",
        "            total_loss = reconstruction_loss + kl_loss * 0.1 + sparsity_loss * 0.01\n",
        "\n",
        "            return total_loss\n",
        "\n",
        "        self.vae_model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
        "            loss=advanced_vae_loss\n",
        "        )\n",
        "\n",
        "        print(\"✅ Advanced VAE built with skip connections and enhanced loss!\")\n",
        "\n",
        "    def train_advanced_vae(self, epochs: int = 100, batch_size: int = 64) -> Any:\n",
        "        \"\"\"Train VAE with advanced techniques.\"\"\"\n",
        "        print(\"🏋️ Training Advanced VAE...\")\n",
        "\n",
        "        # Get minority class data for training\n",
        "        minority_mask = self.original_data['y'] == 0\n",
        "        minority_data = self.original_data['X_flat'][minority_mask]\n",
        "\n",
        "        print(f\"Training VAE on {len(minority_data)} minority samples\")\n",
        "\n",
        "        # Advanced callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6),\n",
        "            ModelCheckpoint('best_vae.h5', save_best_only=True, monitor='val_loss')\n",
        "        ]\n",
        "\n",
        "        history = self.vae_model.fit(\n",
        "            minority_data, minority_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"✅ Advanced VAE training completed!\")\n",
        "        return history\n",
        "\n",
        "    def generate_high_quality_synthetic_data(self, strategy='adaptive') -> None:\n",
        "        \"\"\"Generate high-quality synthetic data with multiple strategies.\"\"\"\n",
        "        print(\"⚡ Generating high-quality synthetic data...\")\n",
        "\n",
        "        class_dist = self.original_data['class_distribution']\n",
        "        minority_count = class_dist.get(0, 0)\n",
        "        majority_count = class_dist.get(1, 0)\n",
        "\n",
        "        if strategy == 'adaptive':\n",
        "            # Generate more synthetic data than needed, then select best samples\n",
        "            synthetic_needed = majority_count - minority_count\n",
        "            generate_count = int(synthetic_needed * 1.5)  # Generate 50% more\n",
        "        else:\n",
        "            generate_count = majority_count - minority_count\n",
        "\n",
        "        print(f\"Generating {generate_count} synthetic samples\")\n",
        "\n",
        "        # Generate with multiple sampling strategies\n",
        "        latent_dim = 64\n",
        "        synthetic_samples = []\n",
        "\n",
        "        # Strategy 1: Random sampling from learned distribution\n",
        "        random_latent = np.random.normal(0, 1, (generate_count // 3, latent_dim))\n",
        "        synthetic_1 = self.decoder.predict(random_latent, verbose=0)\n",
        "\n",
        "        # Strategy 2: Interpolation between real samples\n",
        "        minority_mask = self.original_data['y'] == 0\n",
        "        real_minority = self.original_data['X_flat'][minority_mask]\n",
        "        real_encoded = self.encoder.predict(real_minority, verbose=0)[0]  # Get mean\n",
        "\n",
        "        # Create interpolations\n",
        "        interpolation_count = generate_count // 3\n",
        "        interpolated_latent = []\n",
        "        for _ in range(interpolation_count):\n",
        "            idx1, idx2 = np.random.choice(len(real_encoded), 2, replace=False)\n",
        "            alpha = np.random.beta(2, 2)  # Beta distribution for smooth interpolation\n",
        "            interpolated = alpha * real_encoded[idx1] + (1 - alpha) * real_encoded[idx2]\n",
        "            interpolated_latent.append(interpolated)\n",
        "\n",
        "        synthetic_2 = self.decoder.predict(np.array(interpolated_latent), verbose=0)\n",
        "\n",
        "        # Strategy 3: Perturbed real samples\n",
        "        perturbation_count = generate_count - len(synthetic_1) - len(synthetic_2)\n",
        "        perturbed_latent = real_encoded[:perturbation_count] + np.random.normal(0, 0.1, (perturbation_count, latent_dim))\n",
        "        synthetic_3 = self.decoder.predict(perturbed_latent, verbose=0)\n",
        "\n",
        "        # Combine all synthetic data\n",
        "        all_synthetic = np.vstack([synthetic_1, synthetic_2, synthetic_3])\n",
        "\n",
        "        if strategy == 'adaptive':\n",
        "            # Select best synthetic samples based on similarity to real minority class\n",
        "            from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "            # Calculate similarity to real minority samples\n",
        "            similarities = cosine_similarity(all_synthetic, real_minority)\n",
        "            avg_similarities = similarities.mean(axis=1)\n",
        "\n",
        "            # Select top samples\n",
        "            top_indices = np.argsort(avg_similarities)[-synthetic_needed:]\n",
        "            selected_synthetic = all_synthetic[top_indices]\n",
        "        else:\n",
        "            selected_synthetic = all_synthetic\n",
        "\n",
        "        # Create balanced dataset\n",
        "        minority_data_lstm = self.original_data['X_lstm'][minority_mask]\n",
        "        majority_data_lstm = self.original_data['X_lstm'][~minority_mask]\n",
        "\n",
        "        # Reshape synthetic data for LSTM\n",
        "        synthetic_lstm = selected_synthetic.reshape(len(selected_synthetic), 30, 7)\n",
        "\n",
        "        # Combine data\n",
        "        all_minority_lstm = np.vstack([minority_data_lstm, synthetic_lstm])\n",
        "        balanced_X = np.vstack([all_minority_lstm, majority_data_lstm])\n",
        "        balanced_y = np.hstack([\n",
        "            np.zeros(len(all_minority_lstm)),\n",
        "            np.ones(len(majority_data_lstm))\n",
        "        ])\n",
        "\n",
        "        # Also create enhanced features for synthetic data\n",
        "        synthetic_flat_original_scale = self.scaler.inverse_transform(selected_synthetic)\n",
        "        synthetic_enhanced = self._create_enhanced_features(synthetic_flat_original_scale)\n",
        "        synthetic_enhanced_normalized = self.feature_scaler.transform(synthetic_enhanced)\n",
        "\n",
        "        # Combine enhanced features\n",
        "        minority_enhanced = self.original_data['X_enhanced'][minority_mask]\n",
        "        majority_enhanced = self.original_data['X_enhanced'][~minority_mask]\n",
        "\n",
        "        all_minority_enhanced = np.vstack([minority_enhanced, synthetic_enhanced_normalized])\n",
        "        balanced_X_enhanced = np.vstack([all_minority_enhanced, majority_enhanced])\n",
        "\n",
        "        # Shuffle\n",
        "        shuffle_idx = np.random.permutation(len(balanced_X))\n",
        "        balanced_X = balanced_X[shuffle_idx]\n",
        "        balanced_X_enhanced = balanced_X_enhanced[shuffle_idx]\n",
        "        balanced_y = balanced_y[shuffle_idx]\n",
        "\n",
        "        self.enhanced_data = {\n",
        "            'X_lstm': balanced_X,\n",
        "            'X_enhanced': balanced_X_enhanced,\n",
        "            'y': balanced_y\n",
        "        }\n",
        "\n",
        "        print(f\"✅ High-quality synthetic data generated!\")\n",
        "        print(f\"   Balanced dataset: {len(balanced_X)} samples\")\n",
        "        print(f\"   Non-dropout: {np.sum(balanced_y == 0)} ({np.mean(balanced_y == 0)*100:.1f}%)\")\n",
        "        print(f\"   Dropout: {np.sum(balanced_y == 1)} ({np.mean(balanced_y == 1)*100:.1f}%)\")\n",
        "\n",
        "    def build_ensemble_models(self) -> None:\n",
        "        \"\"\"Build multiple advanced model architectures.\"\"\"\n",
        "        print(\"🔮 Building ensemble of advanced models...\")\n",
        "\n",
        "        # Model 1: Advanced LSTM with attention\n",
        "        def build_attention_lstm():\n",
        "            inputs = keras.Input(shape=(30, 7))\n",
        "\n",
        "            # Multi-head LSTM layers\n",
        "            lstm1 = layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(inputs)\n",
        "            lstm1 = layers.BatchNormalization()(lstm1)\n",
        "\n",
        "            lstm2 = layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(lstm1)\n",
        "            lstm2 = layers.BatchNormalization()(lstm2)\n",
        "\n",
        "            # Attention mechanism\n",
        "            attention = layers.Dense(1, activation='tanh')(lstm2)\n",
        "            attention = layers.Flatten()(attention)\n",
        "            attention = layers.Activation('softmax')(attention)\n",
        "            attention = layers.RepeatVector(64)(attention)\n",
        "            attention = layers.Permute([2, 1])(attention)\n",
        "\n",
        "            # Apply attention weights\n",
        "            attended = layers.Multiply()([lstm2, attention])\n",
        "            attended = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(attended)\n",
        "\n",
        "            # Dense layers\n",
        "            dense1 = layers.Dense(32, activation='relu')(attended)\n",
        "            dense1 = layers.BatchNormalization()(dense1)\n",
        "            dense1 = layers.Dropout(0.4)(dense1)\n",
        "\n",
        "            output = layers.Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "            model = Model(inputs, output)\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', 'precision', 'recall']\n",
        "            )\n",
        "            return model\n",
        "\n",
        "        # Model 2: CNN-LSTM hybrid\n",
        "        def build_cnn_lstm():\n",
        "            inputs = keras.Input(shape=(30, 7))\n",
        "\n",
        "            # Expand dimensions for Conv1D\n",
        "            x = layers.Reshape((30, 7, 1))(inputs)\n",
        "\n",
        "            # Conv1D layers for feature extraction\n",
        "            conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "            conv1 = layers.BatchNormalization()(conv1)\n",
        "            conv1 = layers.MaxPooling2D((2, 1))(conv1)\n",
        "\n",
        "            conv2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "            conv2 = layers.BatchNormalization()(conv2)\n",
        "            conv2 = layers.MaxPooling2D((2, 1))(conv2)\n",
        "\n",
        "            # Reshape for LSTM\n",
        "            conv_shape = conv2.shape\n",
        "            reshaped = layers.Reshape((conv_shape[1], conv_shape[2] * conv_shape[3]))(conv2)\n",
        "\n",
        "            # LSTM layers\n",
        "            lstm = layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)(reshaped)\n",
        "            lstm = layers.BatchNormalization()(lstm)\n",
        "\n",
        "            dense = layers.Dense(32, activation='relu')(lstm)\n",
        "            dense = layers.Dropout(0.4)(dense)\n",
        "\n",
        "            output = layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "            model = Model(inputs, output)\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', 'precision', 'recall']\n",
        "            )\n",
        "            return model\n",
        "\n",
        "        # Model 3: Enhanced features model (for engineered features)\n",
        "        def build_enhanced_model():\n",
        "            inputs = keras.Input(shape=(self.original_data['X_enhanced'].shape[1],))\n",
        "\n",
        "            # Deep neural network with residual connections\n",
        "            x1 = layers.Dense(256, activation='relu')(inputs)\n",
        "            x1 = layers.BatchNormalization()(x1)\n",
        "            x1 = layers.Dropout(0.3)(x1)\n",
        "\n",
        "            x2 = layers.Dense(128, activation='relu')(x1)\n",
        "            x2 = layers.BatchNormalization()(x2)\n",
        "            x2 = layers.Dropout(0.3)(x2)\n",
        "\n",
        "            # Residual connection\n",
        "            x_res = layers.Dense(128, activation='relu')(inputs)\n",
        "            x2 = layers.Add()([x2, x_res])\n",
        "\n",
        "            x3 = layers.Dense(64, activation='relu')(x2)\n",
        "            x3 = layers.BatchNormalization()(x3)\n",
        "            x3 = layers.Dropout(0.4)(x3)\n",
        "\n",
        "            x4 = layers.Dense(32, activation='relu')(x3)\n",
        "            x4 = layers.Dropout(0.4)(x4)\n",
        "\n",
        "            output = layers.Dense(1, activation='sigmoid')(x4)\n",
        "\n",
        "            model = Model(inputs, output)\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', 'precision', 'recall']\n",
        "            )\n",
        "            return model\n",
        "\n",
        "        # Model 4: Bidirectional LSTM\n",
        "        def build_bidirectional_lstm():\n",
        "            inputs = keras.Input(shape=(30, 7))\n",
        "\n",
        "            # Bidirectional LSTM layers\n",
        "            bilstm1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.3))(inputs)\n",
        "            bilstm1 = layers.BatchNormalization()(bilstm1)\n",
        "\n",
        "            bilstm2 = layers.Bidirectional(layers.LSTM(32, dropout=0.3))(bilstm1)\n",
        "            bilstm2 = layers.BatchNormalization()(bilstm2)\n",
        "\n",
        "            dense1 = layers.Dense(32, activation='relu')(bilstm2)\n",
        "            dense1 = layers.Dropout(0.4)(dense1)\n",
        "\n",
        "            output = layers.Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "            model = Model(inputs, output)\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', 'precision', 'recall']\n",
        "            )\n",
        "            return model\n",
        "\n",
        "        # Build all models\n",
        "        self.models = {\n",
        "            'attention_lstm': build_attention_lstm(),\n",
        "            'cnn_lstm': build_cnn_lstm(),\n",
        "            'enhanced_features': build_enhanced_model(),\n",
        "            'bidirectional_lstm': build_bidirectional_lstm()\n",
        "        }\n",
        "\n",
        "        print(f\"✅ Built {len(self.models)} advanced models!\")\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"   {name}: {model.count_params():,} parameters\")\n",
        "\n",
        "    def train_ensemble_with_cv(self, epochs: int = 50, cv_folds: int = 5) -> Dict:\n",
        "        \"\"\"Train ensemble models with cross-validation.\"\"\"\n",
        "        print(\"🏋️ Training ensemble models with cross-validation...\")\n",
        "\n",
        "        # Use balanced data\n",
        "        if self.enhanced_data is None:\n",
        "            raise ValueError(\"Must generate synthetic data first!\")\n",
        "\n",
        "        X_lstm = self.enhanced_data['X_lstm']\n",
        "        X_enhanced = self.enhanced_data['X_enhanced']\n",
        "        y = self.enhanced_data['y']\n",
        "\n",
        "        # Cross-validation setup\n",
        "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "            fold_scores = []\n",
        "            fold_histories = []\n",
        "\n",
        "            for fold, (train_idx, val_idx) in enumerate(skf.split(X_lstm, y)):\n",
        "                print(f\"  Fold {fold + 1}/{cv_folds}\")\n",
        "\n",
        "                # Prepare data based on model type\n",
        "                if model_name == 'enhanced_features':\n",
        "                    X_train, X_val = X_enhanced[train_idx], X_enhanced[val_idx]\n",
        "                else:\n",
        "                    X_train, X_val = X_lstm[train_idx], X_lstm[val_idx]\n",
        "\n",
        "                y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                # Clone model for this fold\n",
        "                model_clone = keras.models.clone_model(model)\n",
        "                model_clone.compile(\n",
        "                    optimizer=model.optimizer,\n",
        "                    loss=model.loss,\n",
        "                    metrics=model.metrics\n",
        "                )\n",
        "\n",
        "                # Advanced callbacks\n",
        "                callbacks = [\n",
        "                    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
        "                    ReduceLROnPlateau(monitor='val_accuracy', factor=0.7, patience=5, min_lr=1e-6)\n",
        "                ]\n",
        "\n",
        "                # Train model\n",
        "                history = model_clone.fit(\n",
        "                    X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Evaluate fold\n",
        "                val_accuracy = max(history.history['val_accuracy'])\n",
        "                fold_scores.append(val_accuracy)\n",
        "                fold_histories.append(history)\n",
        "\n",
        "                print(f\"    Fold {fold + 1} accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Store results\n",
        "            avg_score = np.mean(fold_scores)\n",
        "            std_score = np.std(fold_scores)\n",
        "\n",
        "            results[model_name] = {\n",
        "                'mean_accuracy': avg_score,\n",
        "                'std_accuracy': std_score,\n",
        "                'fold_scores': fold_scores,\n",
        "                'fold_histories': fold_histories\n",
        "            }\n",
        "\n",
        "            print(f\"  {model_name} CV Accuracy: {avg_score:.4f} (±{std_score:.4f})\")\n",
        "\n",
        "        # Train final models on full dataset\n",
        "        print(\"\\nTraining final models on full dataset...\")\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"Training final {model_name}...\")\n",
        "\n",
        "            if model_name == 'enhanced_features':\n",
        "                X_train = X_enhanced\n",
        "            else:\n",
        "                X_train = X_lstm\n",
        "\n",
        "            callbacks = [\n",
        "                EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True),\n",
        "                ReduceLROnPlateau(monitor='val_accuracy', factor=0.7, patience=7, min_lr=1e-6),\n",
        "                ModelCheckpoint(f'best_{model_name}.h5', save_best_only=True, monitor='val_accuracy')\n",
        "            ]\n",
        "\n",
        "            model.fit(\n",
        "                X_train, y,\n",
        "                epochs=epochs,\n",
        "                batch_size=64,\n",
        "                validation_split=0.2,\n",
        "                callbacks=callbacks,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "        print(\"✅ Ensemble training completed!\")\n",
        "        return results\n",
        "\n",
        "    def evaluate_ensemble(self, X_test=None, y_test=None) -> Dict:\n",
        "        \"\"\"Evaluate ensemble models with comprehensive metrics.\"\"\"\n",
        "        print(\"📈 Evaluating ensemble models...\")\n",
        "\n",
        "        # Use original data for fair evaluation\n",
        "        if X_test is None or y_test is None:\n",
        "            X_test_lstm = self.original_data['X_lstm']\n",
        "            X_test_enhanced = self.original_data['X_enhanced']\n",
        "            y_test = self.original_data['y']\n",
        "\n",
        "        results = {}\n",
        "        all_predictions = {}\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "            # Choose appropriate test data\n",
        "            if model_name == 'enhanced_features':\n",
        "                X_test_model = X_test_enhanced\n",
        "            else:\n",
        "                X_test_model = X_test_lstm\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred_proba = model.predict(X_test_model, verbose=0).flatten()\n",
        "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "            # Calculate metrics\n",
        "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'precision': precision_score(y_test, y_pred),\n",
        "                'recall': recall_score(y_test, y_pred),\n",
        "                'f1_score': f1_score(y_test, y_pred),\n",
        "                'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
        "            }\n",
        "\n",
        "            results[model_name] = metrics\n",
        "            all_predictions[model_name] = y_pred_proba\n",
        "\n",
        "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
        "            print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Ensemble predictions\n",
        "        print(\"\\nComputing ensemble predictions...\")\n",
        "\n",
        "        # Simple average ensemble\n",
        "        ensemble_proba = np.mean(list(all_predictions.values()), axis=0)\n",
        "        ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
        "\n",
        "        ensemble_metrics = {\n",
        "            'accuracy': accuracy_score(y_test, ensemble_pred),\n",
        "            'precision': precision_score(y_test, ensemble_pred),\n",
        "            'recall': recall_score(y_test, ensemble_pred),\n",
        "            'f1_score': f1_score(y_test, ensemble_pred),\n",
        "            'roc_auc': roc_auc_score(y_test, ensemble_proba)\n",
        "        }\n",
        "\n",
        "        results['ensemble'] = ensemble_metrics\n",
        "\n",
        "        print(f\"\\n🎯 ENSEMBLE RESULTS:\")\n",
        "        print(f\"  Accuracy: {ensemble_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Precision: {ensemble_metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall: {ensemble_metrics['recall']:.4f}\")\n",
        "        print(f\"  F1-Score: {ensemble_metrics['f1_score']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {ensemble_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(\"\\n📊 Detailed Classification Report (Ensemble):\")\n",
        "        print(classification_report(y_test, ensemble_pred, target_names=['Non-Dropout', 'Dropout']))\n",
        "\n",
        "        return results, ensemble_proba, ensemble_pred\n",
        "\n",
        "    def save_models(self, base_path: str = './models/'):\n",
        "        \"\"\"Save all trained models.\"\"\"\n",
        "        import os\n",
        "        os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "        if self.vae_model:\n",
        "            self.vae_model.save(f'{base_path}advanced_vae.h5')\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            model.save(f'{base_path}advanced_{name}.h5')\n",
        "\n",
        "        print(f\"✅ All models saved to {base_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Enhanced main function with comprehensive pipeline.\"\"\"\n",
        "    print(\"🎓 ADVANCED Student Dropout Prediction with Ensemble VAE + LSTM\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Initialize predictor\n",
        "    predictor = AdvancedStudentDropoutPredictor()\n",
        "\n",
        "    try:\n",
        "        # Step 1: Load and preprocess data with advanced feature engineering\n",
        "        predictor.load_and_preprocess_data('model1_210_features.csv')\n",
        "\n",
        "        # Step 2: Build and train advanced VAE\n",
        "        input_dim = predictor.original_data['X_flat'].shape[1]\n",
        "        predictor.build_advanced_vae(input_dim, latent_dim=64)\n",
        "        vae_history = predictor.train_advanced_vae(epochs=80, batch_size=64)\n",
        "\n",
        "        # Step 3: Generate high-quality synthetic data\n",
        "        predictor.generate_high_quality_synthetic_data(strategy='adaptive')\n",
        "\n",
        "        # Step 4: Build ensemble of advanced models\n",
        "        predictor.build_ensemble_models()\n",
        "\n",
        "        # Step 5: Train ensemble with cross-validation\n",
        "        cv_results = predictor.train_ensemble_with_cv(epochs=60, cv_folds=5)\n",
        "\n",
        "        # Step 6: Comprehensive evaluation\n",
        "        results, ensemble_proba, ensemble_pred = predictor.evaluate_ensemble()\n",
        "\n",
        "        # Step 7: Save models\n",
        "        predictor.save_models()\n",
        "\n",
        "        # Step 8: Print final summary\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"🏆 FINAL RESULTS SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        best_single_model = max(results.items(), key=lambda x: x[1]['accuracy'] if x[0] != 'ensemble' else 0)\n",
        "\n",
        "        print(f\"Best Single Model: {best_single_model[0]} - Accuracy: {best_single_model[1]['accuracy']:.4f}\")\n",
        "        print(f\"Ensemble Model Accuracy: {results['ensemble']['accuracy']:.4f}\")\n",
        "        print(f\"Improvement over best single: {results['ensemble']['accuracy'] - best_single_model[1]['accuracy']:.4f}\")\n",
        "\n",
        "        print(\"\\n✅ Advanced pipeline completed successfully!\")\n",
        "\n",
        "        return predictor, results\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ CSV file not found. Please ensure 'model1_210_features_spliting.csv' is in the current directory.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during execution: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Enhanced requirements\n",
        "    enhanced_packages = [\n",
        "        \"tensorflow>=2.10.0\",\n",
        "        \"pandas>=1.3.0\",\n",
        "        \"numpy>=1.21.0\",\n",
        "        \"scikit-learn>=1.0.0\",\n",
        "        \"matplotlib>=3.3.0\",\n",
        "        \"seaborn>=0.11.0\"\n",
        "    ]\n",
        "\n",
        "    print(\"📦 Enhanced packages required:\")\n",
        "    for package in enhanced_packages:\n",
        "        print(f\"   - {package}\")\n",
        "    print(\"\\nInstall with: pip install tensorflow pandas numpy scikit-learn matplotlib seaborn\")\n",
        "    print()\n",
        "\n",
        "    predictor, results = main()\n"
      ]
    }
  ]
}