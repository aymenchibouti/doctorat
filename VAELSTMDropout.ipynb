{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCTfKzQeJrrA+OiXwGCbtR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/doctorat/blob/main/VAELSTMDropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBGFCdGOJPGA",
        "outputId": "0891ee5d-f17c-4581-f808-2e833bd2c6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "STARTING VAE-LSTM DROPOUT PREDICTION PIPELINE\n",
            "==================================================\n",
            "Loading and preprocessing data...\n",
            "Loaded 120542 samples with 214 columns\n",
            "Class distribution: {np.int64(0): np.int64(24961), np.int64(1): np.int64(95581)}\n",
            "Dropout samples: 95581\n",
            "No dropout samples: 24961\n",
            "Imbalance ratio: 3.83\n",
            "Building VAE model...\n",
            "VAE model built successfully!\n",
            "Training VAE on minority class (no dropout)...\n",
            "Epoch 1/50\n",
            "An error occurred: Tried to convert 'x' to a tensor and failed. Error: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n",
            "\n",
            "```\n",
            "x = Input(...)\n",
            "...\n",
            "tf_fn(x)  # Invalid.\n",
            "```\n",
            "\n",
            "What you should do instead is wrap `tf_fn` in a layer:\n",
            "\n",
            "```\n",
            "class MyLayer(Layer):\n",
            "    def call(self, x):\n",
            "        return tf_fn(x)\n",
            "\n",
            "x = MyLayer()(x)\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, optimizers, losses\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class VAELSTMDropoutPredictor:\n",
        "    def __init__(self, sequence_length=30, features_per_day=7, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the VAE-LSTM Dropout Predictor\n",
        "\n",
        "        Args:\n",
        "            sequence_length (int): Number of days in sequence (30)\n",
        "            features_per_day (int): Number of features per day (7)\n",
        "            latent_dim (int): Latent dimension for VAE\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.features_per_day = features_per_day\n",
        "        self.latent_dim = latent_dim\n",
        "        self.input_shape = (sequence_length, features_per_day)\n",
        "\n",
        "        # Models\n",
        "        self.vae = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.lstm_model = None\n",
        "\n",
        "        # Data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.original_data = None\n",
        "        self.augmented_data = None\n",
        "        self.history = {}\n",
        "\n",
        "    def load_and_preprocess_data(self, csv_file_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess the student activity data\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file\n",
        "        \"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
        "\n",
        "        # Extract feature columns (day_1_access to day_30_video)\n",
        "        feature_columns = []\n",
        "        for day in range(1, 31):  # 30 days\n",
        "            feature_columns.extend([\n",
        "                f'day_{day}_access',\n",
        "                f'day_{day}_problem',\n",
        "                f'day_{day}_wiki',\n",
        "                f'day_{day}_discussion',\n",
        "                f'day_{day}_navigate',\n",
        "                f'day_{day}_page_close',\n",
        "                f'day_{day}_video'\n",
        "            ])\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = df[feature_columns].values\n",
        "        y = df['dropout'].values\n",
        "\n",
        "        # Reshape data for LSTM (samples, timesteps, features)\n",
        "        X_sequences = X.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Normalize features\n",
        "        X_flat = X_sequences.reshape(-1, self.sequence_length * self.features_per_day)\n",
        "        X_normalized = self.scaler.fit_transform(X_flat)\n",
        "        X_sequences = X_normalized.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Analyze class distribution\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        class_distribution = dict(zip(unique, counts))\n",
        "        print(f\"Class distribution: {class_distribution}\")\n",
        "\n",
        "        # Separate by class\n",
        "        dropout_indices = np.where(y == 1)[0]\n",
        "        no_dropout_indices = np.where(y == 0)[0]\n",
        "\n",
        "        self.dropout_sequences = X_sequences[dropout_indices]\n",
        "        self.no_dropout_sequences = X_sequences[no_dropout_indices]\n",
        "\n",
        "        print(f\"Dropout samples: {len(self.dropout_sequences)}\")\n",
        "        print(f\"No dropout samples: {len(self.no_dropout_sequences)}\")\n",
        "        print(f\"Imbalance ratio: {len(self.dropout_sequences) / len(self.no_dropout_sequences):.2f}\")\n",
        "\n",
        "        self.original_data = {\n",
        "            'X': X_sequences,\n",
        "            'y': y,\n",
        "            'class_distribution': class_distribution\n",
        "        }\n",
        "\n",
        "        return X_sequences, y\n",
        "\n",
        "    def build_vae(self):\n",
        "        \"\"\"\n",
        "        Build Variational Autoencoder for data augmentation\n",
        "        \"\"\"\n",
        "        print(\"Building VAE model...\")\n",
        "\n",
        "        # Encoder\n",
        "        encoder_inputs = layers.Input(shape=self.input_shape, name='encoder_input')\n",
        "        x = layers.Flatten()(encoder_inputs)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "        # Latent space\n",
        "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        # Sampling function\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = layers.Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "        # Create encoder model\n",
        "        self.encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "        # Decoder\n",
        "        decoder_inputs = layers.Input(shape=(self.latent_dim,), name='decoder_input')\n",
        "        x = layers.Dense(128, activation='relu')(decoder_inputs)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dense(self.sequence_length * self.features_per_day, activation='sigmoid')(x)\n",
        "        decoder_outputs = layers.Reshape(self.input_shape)(x)\n",
        "\n",
        "        # Create decoder model\n",
        "        self.decoder = Model(decoder_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "        # VAE model\n",
        "        vae_outputs = self.decoder(self.encoder(encoder_inputs)[2])\n",
        "        self.vae = Model(encoder_inputs, vae_outputs, name='vae')\n",
        "\n",
        "        # VAE loss function\n",
        "        def vae_loss(x, x_decoded_mean):\n",
        "            reconstruction_loss = losses.mse(tf.keras.backend.flatten(x),\n",
        "                                           tf.keras.backend.flatten(x_decoded_mean))\n",
        "            reconstruction_loss *= self.sequence_length * self.features_per_day\n",
        "\n",
        "            kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
        "            kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
        "            kl_loss *= -0.5\n",
        "\n",
        "            return tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "        self.vae.compile(optimizer='adam', loss=vae_loss)\n",
        "\n",
        "        print(\"VAE model built successfully!\")\n",
        "        return self.vae\n",
        "\n",
        "    def train_vae(self, epochs=100, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the VAE on minority class data\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        print(\"Training VAE on minority class (no dropout)...\")\n",
        "\n",
        "        # Use minority class for VAE training\n",
        "        minority_data = self.no_dropout_sequences\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(patience=15, restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10)\n",
        "\n",
        "        # Train VAE\n",
        "        vae_history = self.vae.fit(\n",
        "            minority_data, minority_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['vae'] = vae_history.history\n",
        "        print(\"VAE training completed!\")\n",
        "\n",
        "        return vae_history\n",
        "\n",
        "    def generate_synthetic_data(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate synthetic data using trained VAE\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): Number of synthetic samples to generate\n",
        "        \"\"\"\n",
        "        print(f\"Generating {num_samples} synthetic samples...\")\n",
        "\n",
        "        # Sample from latent space\n",
        "        random_latent_vectors = np.random.normal(size=(num_samples, self.latent_dim))\n",
        "\n",
        "        # Add some structure by sampling around the learned distribution\n",
        "        if len(self.no_dropout_sequences) > 0:\n",
        "            # Encode some real samples to get latent distribution\n",
        "            z_mean, z_log_var, _ = self.encoder.predict(self.no_dropout_sequences[:100])\n",
        "\n",
        "            # Sample around the mean of the latent distribution\n",
        "            latent_mean = np.mean(z_mean, axis=0)\n",
        "            latent_std = np.std(z_mean, axis=0)\n",
        "\n",
        "            # Generate samples around the learned distribution\n",
        "            random_latent_vectors = np.random.normal(\n",
        "                loc=latent_mean,\n",
        "                scale=latent_std * 1.5,  # Add some variability\n",
        "                size=(num_samples, self.latent_dim)\n",
        "            )\n",
        "\n",
        "        # Decode to generate synthetic data\n",
        "        synthetic_data = self.decoder.predict(random_latent_vectors)\n",
        "\n",
        "        print(f\"Generated {len(synthetic_data)} synthetic samples\")\n",
        "        return synthetic_data\n",
        "\n",
        "    def create_balanced_dataset(self):\n",
        "        \"\"\"\n",
        "        Create balanced dataset using original + synthetic data\n",
        "        \"\"\"\n",
        "        print(\"Creating balanced dataset...\")\n",
        "\n",
        "        # Calculate how many synthetic samples needed\n",
        "        majority_count = len(self.dropout_sequences)\n",
        "        minority_count = len(self.no_dropout_sequences)\n",
        "        samples_needed = majority_count - minority_count\n",
        "\n",
        "        if samples_needed > 0:\n",
        "            # Generate synthetic data\n",
        "            synthetic_data = self.generate_synthetic_data(samples_needed)\n",
        "\n",
        "            # Combine original and synthetic data\n",
        "            balanced_X = np.vstack([\n",
        "                self.original_data['X'],  # Original data\n",
        "                synthetic_data  # Synthetic minority samples\n",
        "            ])\n",
        "\n",
        "            # Create corresponding labels\n",
        "            balanced_y = np.hstack([\n",
        "                self.original_data['y'],  # Original labels\n",
        "                np.zeros(samples_needed)  # Synthetic minority labels (0 = no dropout)\n",
        "            ])\n",
        "\n",
        "        else:\n",
        "            print(\"Dataset is already balanced or minority is larger\")\n",
        "            balanced_X = self.original_data['X']\n",
        "            balanced_y = self.original_data['y']\n",
        "\n",
        "        print(f\"Balanced dataset size: {len(balanced_X)}\")\n",
        "        print(f\"Class distribution after balancing: {np.unique(balanced_y, return_counts=True)}\")\n",
        "\n",
        "        self.augmented_data = {\n",
        "            'X': balanced_X,\n",
        "            'y': balanced_y,\n",
        "            'synthetic_samples': samples_needed if samples_needed > 0 else 0\n",
        "        }\n",
        "\n",
        "        return balanced_X, balanced_y\n",
        "\n",
        "    def build_lstm_model(self):\n",
        "        \"\"\"\n",
        "        Build LSTM model for dropout prediction\n",
        "        \"\"\"\n",
        "        print(\"Building LSTM model...\")\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            # First LSTM layer\n",
        "            layers.LSTM(64,\n",
        "                       return_sequences=True,\n",
        "                       input_shape=self.input_shape,\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2),\n",
        "\n",
        "            # Second LSTM layer\n",
        "            layers.LSTM(32,\n",
        "                       return_sequences=False,\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2),\n",
        "\n",
        "            # Dense layers\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(8, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        self.lstm_model = model\n",
        "        print(\"LSTM model built successfully!\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lstm(self, epochs=100, batch_size=64):\n",
        "        \"\"\"\n",
        "        Train LSTM model on balanced dataset\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        print(\"Training LSTM model...\")\n",
        "\n",
        "        if self.augmented_data is None:\n",
        "            print(\"Creating balanced dataset first...\")\n",
        "            self.create_balanced_dataset()\n",
        "\n",
        "        X = self.augmented_data['X']\n",
        "        y = self.augmented_data['y']\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10)\n",
        "\n",
        "        # Train model\n",
        "        lstm_history = self.lstm_model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['lstm'] = lstm_history.history\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_predictions = self.lstm_model.predict(X_test)\n",
        "        test_predictions_binary = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        self.test_metrics = {\n",
        "            'accuracy': accuracy_score(y_test, test_predictions_binary),\n",
        "            'precision': precision_score(y_test, test_predictions_binary),\n",
        "            'recall': recall_score(y_test, test_predictions_binary),\n",
        "            'f1_score': f1_score(y_test, test_predictions_binary)\n",
        "        }\n",
        "\n",
        "        print(\"\\nLSTM training completed!\")\n",
        "        print(\"\\nTest Set Performance:\")\n",
        "        for metric, value in self.test_metrics.items():\n",
        "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "        return lstm_history, X_test, y_test, test_predictions\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training history for both VAE and LSTM\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # VAE Loss\n",
        "        if 'vae' in self.history:\n",
        "            axes[0, 0].plot(self.history['vae']['loss'], label='Training Loss')\n",
        "            axes[0, 0].plot(self.history['vae']['val_loss'], label='Validation Loss')\n",
        "            axes[0, 0].set_title('VAE Training Loss')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True)\n",
        "\n",
        "        # LSTM Loss\n",
        "        if 'lstm' in self.history:\n",
        "            axes[0, 1].plot(self.history['lstm']['loss'], label='Training Loss')\n",
        "            axes[0, 1].plot(self.history['lstm']['val_loss'], label='Validation Loss')\n",
        "            axes[0, 1].set_title('LSTM Training Loss')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Loss')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "\n",
        "        # LSTM Accuracy\n",
        "        if 'lstm' in self.history:\n",
        "            axes[1, 0].plot(self.history['lstm']['accuracy'], label='Training Accuracy')\n",
        "            axes[1, 0].plot(self.history['lstm']['val_accuracy'], label='Validation Accuracy')\n",
        "            axes[1, 0].set_title('LSTM Training Accuracy')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Accuracy')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True)\n",
        "\n",
        "        # Model Performance Metrics\n",
        "        if hasattr(self, 'test_metrics'):\n",
        "            metrics_names = list(self.test_metrics.keys())\n",
        "            metrics_values = list(self.test_metrics.values())\n",
        "\n",
        "            axes[1, 1].bar(metrics_names, metrics_values)\n",
        "            axes[1, 1].set_title('Test Set Performance Metrics')\n",
        "            axes[1, 1].set_ylabel('Score')\n",
        "            axes[1, 1].set_ylim(0, 1)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for i, v in enumerate(metrics_values):\n",
        "                axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix\n",
        "        \"\"\"\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "        cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['No Dropout', 'Dropout'],\n",
        "                    yticklabels=['No Dropout', 'Dropout'])\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred_binary,\n",
        "                                  target_names=['No Dropout', 'Dropout']))\n",
        "\n",
        "    def predict_dropout(self, student_sequences):\n",
        "        \"\"\"\n",
        "        Predict dropout probability for new student data\n",
        "\n",
        "        Args:\n",
        "            student_sequences: Array of shape (n_students, 30, 7)\n",
        "\n",
        "        Returns:\n",
        "            Dropout probabilities\n",
        "        \"\"\"\n",
        "        if self.lstm_model is None:\n",
        "            raise ValueError(\"LSTM model not trained yet!\")\n",
        "\n",
        "        # Normalize the input sequences\n",
        "        sequences_flat = student_sequences.reshape(-1, self.sequence_length * self.features_per_day)\n",
        "        sequences_normalized = self.scaler.transform(sequences_flat)\n",
        "        sequences_reshaped = sequences_normalized.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.lstm_model.predict(sequences_reshaped)\n",
        "\n",
        "        return predictions.flatten()\n",
        "\n",
        "    def run_complete_pipeline(self, csv_file_path, vae_epochs=100, lstm_epochs=100):\n",
        "        \"\"\"\n",
        "        Run the complete VAE-LSTM pipeline\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file\n",
        "            vae_epochs (int): Epochs for VAE training\n",
        "            lstm_epochs (int): Epochs for LSTM training\n",
        "        \"\"\"\n",
        "        print(\"=\"*50)\n",
        "        print(\"STARTING VAE-LSTM DROPOUT PREDICTION PIPELINE\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Step 1: Load and preprocess data\n",
        "        X, y = self.load_and_preprocess_data(csv_file_path)\n",
        "\n",
        "        # Step 2: Build and train VAE\n",
        "        self.build_vae()\n",
        "        self.train_vae(epochs=vae_epochs)\n",
        "\n",
        "        # Step 3: Create balanced dataset\n",
        "        balanced_X, balanced_y = self.create_balanced_dataset()\n",
        "\n",
        "        # Step 4: Build and train LSTM\n",
        "        self.build_lstm_model()\n",
        "        lstm_history, X_test, y_test, test_predictions = self.train_lstm(epochs=lstm_epochs)\n",
        "\n",
        "        # Step 5: Visualize results\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"TRAINING COMPLETED - GENERATING VISUALIZATIONS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        self.plot_training_history()\n",
        "        self.plot_confusion_matrix(y_test, test_predictions)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return self.test_metrics\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the predictor\n",
        "    predictor = VAELSTMDropoutPredictor(\n",
        "        sequence_length=30,\n",
        "        features_per_day=7,\n",
        "        latent_dim=32\n",
        "    )\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    # Replace 'your_data.csv' with the actual path to your CSV file\n",
        "    csv_file_path = 'model1_210_features.csv'\n",
        "\n",
        "    try:\n",
        "        final_metrics = predictor.run_complete_pipeline(\n",
        "            csv_file_path=csv_file_path,\n",
        "            vae_epochs=50,  # Reduce for faster training\n",
        "            lstm_epochs=50\n",
        "        )\n",
        "\n",
        "        print(\"\\nFinal Model Performance:\")\n",
        "        for metric, value in final_metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find the file '{csv_file_path}'\")\n",
        "        print(\"Please make sure the CSV file is in the correct location.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    # Example of making predictions on new data\n",
        "    # Uncomment the following lines if you want to test predictions\n",
        "    \"\"\"\n",
        "    # Create some dummy test data (replace with real data)\n",
        "    test_sequences = np.random.rand(5, 30, 7)  # 5 students, 30 days, 7 features\n",
        "\n",
        "    # Make predictions\n",
        "    dropout_probabilities = predictor.predict_dropout(test_sequences)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i, prob in enumerate(dropout_probabilities):\n",
        "        risk_level = \"High\" if prob > 0.7 else \"Medium\" if prob > 0.3 else \"Low\"\n",
        "        print(f\"Student {i+1}: {prob:.3f} probability ({risk_level} risk)\")\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, optimizers, losses\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class VAELSTMDropoutPredictor:\n",
        "    def __init__(self, sequence_length=30, features_per_day=7, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the VAE-LSTM Dropout Predictor\n",
        "\n",
        "        Args:\n",
        "            sequence_length (int): Number of days in sequence (30)\n",
        "            features_per_day (int): Number of features per day (7)\n",
        "            latent_dim (int): Latent dimension for VAE\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.features_per_day = features_per_day\n",
        "        self.latent_dim = latent_dim\n",
        "        self.input_shape = (sequence_length, features_per_day)\n",
        "\n",
        "        # Models\n",
        "        self.vae = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.lstm_model = None\n",
        "\n",
        "        # Data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.original_data = None\n",
        "        self.augmented_data = None\n",
        "        self.history = {}\n",
        "\n",
        "    def load_and_preprocess_data(self, csv_file_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess the student activity data\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file\n",
        "        \"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
        "\n",
        "        # Extract feature columns (day_1_access to day_30_video)\n",
        "        feature_columns = []\n",
        "        for day in range(1, 31):  # 30 days\n",
        "            feature_columns.extend([\n",
        "                f'day_{day}_access',\n",
        "                f'day_{day}_problem',\n",
        "                f'day_{day}_wiki',\n",
        "                f'day_{day}_discussion',\n",
        "                f'day_{day}_navigate',\n",
        "                f'day_{day}_page_close',\n",
        "                f'day_{day}_video'\n",
        "            ])\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = df[feature_columns].values\n",
        "        y = df['dropout'].values\n",
        "\n",
        "        # Reshape data for LSTM (samples, timesteps, features)\n",
        "        X_sequences = X.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Normalize features\n",
        "        X_flat = X_sequences.reshape(-1, self.sequence_length * self.features_per_day)\n",
        "        X_normalized = self.scaler.fit_transform(X_flat)\n",
        "        X_sequences = X_normalized.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Analyze class distribution\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        class_distribution = dict(zip(unique, counts))\n",
        "        print(f\"Class distribution: {class_distribution}\")\n",
        "\n",
        "        # Separate by class\n",
        "        dropout_indices = np.where(y == 1)[0]\n",
        "        no_dropout_indices = np.where(y == 0)[0]\n",
        "\n",
        "        self.dropout_sequences = X_sequences[dropout_indices]\n",
        "        self.no_dropout_sequences = X_sequences[no_dropout_indices]\n",
        "\n",
        "        print(f\"Dropout samples: {len(self.dropout_sequences)}\")\n",
        "        print(f\"No dropout samples: {len(self.no_dropout_sequences)}\")\n",
        "        print(f\"Imbalance ratio: {len(self.dropout_sequences) / len(self.no_dropout_sequences):.2f}\")\n",
        "\n",
        "        self.original_data = {\n",
        "            'X': X_sequences,\n",
        "            'y': y,\n",
        "            'class_distribution': class_distribution\n",
        "        }\n",
        "\n",
        "        return X_sequences, y\n",
        "\n",
        "    def build_vae(self):\n",
        "        \"\"\"\n",
        "        Build Variational Autoencoder for data augmentation\n",
        "        \"\"\"\n",
        "        print(\"Building VAE model...\")\n",
        "\n",
        "        # Custom sampling layer\n",
        "        class Sampling(layers.Layer):\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var = inputs\n",
        "                batch = tf.shape(z_mean)[0]\n",
        "                dim = tf.shape(z_mean)[1]\n",
        "                epsilon = tf.random.normal(shape=(batch, dim))\n",
        "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        # Encoder\n",
        "        encoder_inputs = layers.Input(shape=self.input_shape, name='encoder_input')\n",
        "        x = layers.Flatten()(encoder_inputs)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "        # Latent space\n",
        "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        # Sampling\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "        # Create encoder model\n",
        "        self.encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "        # Decoder\n",
        "        decoder_inputs = layers.Input(shape=(self.latent_dim,), name='decoder_input')\n",
        "        x = layers.Dense(128, activation='relu')(decoder_inputs)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dense(self.sequence_length * self.features_per_day, activation='sigmoid')(x)\n",
        "        decoder_outputs = layers.Reshape(self.input_shape)(x)\n",
        "\n",
        "        # Create decoder model\n",
        "        self.decoder = Model(decoder_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "        # VAE model\n",
        "        vae_outputs = self.decoder(self.encoder(encoder_inputs)[2])\n",
        "        self.vae = Model(encoder_inputs, vae_outputs, name='vae')\n",
        "\n",
        "        # Custom VAE loss using subclassing\n",
        "        class VAE(Model):\n",
        "            def __init__(self, encoder, decoder, **kwargs):\n",
        "                super(VAE, self).__init__(**kwargs)\n",
        "                self.encoder = encoder\n",
        "                self.decoder = decoder\n",
        "                self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "                self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "                self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "            @property\n",
        "            def metrics(self):\n",
        "                return [\n",
        "                    self.total_loss_tracker,\n",
        "                    self.reconstruction_loss_tracker,\n",
        "                    self.kl_loss_tracker,\n",
        "                ]\n",
        "\n",
        "            def train_step(self, data):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    z_mean, z_log_var, z = self.encoder(data)\n",
        "                    reconstruction = self.decoder(z)\n",
        "\n",
        "                    # Reconstruction loss\n",
        "                    reconstruction_loss = tf.reduce_mean(\n",
        "                        tf.reduce_sum(\n",
        "                            keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    # KL divergence loss\n",
        "                    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "                    total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "                self.total_loss_tracker.update_state(total_loss)\n",
        "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "                self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "                return {\n",
        "                    \"loss\": self.total_loss_tracker.result(),\n",
        "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                    \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                }\n",
        "\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var, z = self.encoder(inputs)\n",
        "                return self.decoder(z)\n",
        "\n",
        "        # Create the custom VAE model\n",
        "        self.vae = VAE(self.encoder, self.decoder)\n",
        "        self.vae.compile(optimizer='adam')\n",
        "\n",
        "        print(\"VAE model built successfully!\")\n",
        "        return self.vae\n",
        "\n",
        "    def train_vae(self, epochs=100, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the VAE on minority class data\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        print(\"Training VAE on minority class (no dropout)...\")\n",
        "\n",
        "        # Use minority class for VAE training\n",
        "        minority_data = self.no_dropout_sequences\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        )\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "            monitor='loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Train VAE\n",
        "        vae_history = self.vae.fit(\n",
        "            minority_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['vae'] = vae_history.history\n",
        "        print(\"VAE training completed!\")\n",
        "\n",
        "        return vae_history\n",
        "\n",
        "    def generate_synthetic_data(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate synthetic data using trained VAE\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): Number of synthetic samples to generate\n",
        "        \"\"\"\n",
        "        print(f\"Generating {num_samples} synthetic samples...\")\n",
        "\n",
        "        # Sample from latent space\n",
        "        random_latent_vectors = np.random.normal(size=(num_samples, self.latent_dim))\n",
        "\n",
        "        # Add some structure by sampling around the learned distribution\n",
        "        if len(self.no_dropout_sequences) > 0:\n",
        "            # Encode some real samples to get latent distribution\n",
        "            z_mean, z_log_var, _ = self.encoder.predict(self.no_dropout_sequences[:100])\n",
        "\n",
        "            # Sample around the mean of the latent distribution\n",
        "            latent_mean = np.mean(z_mean, axis=0)\n",
        "            latent_std = np.std(z_mean, axis=0)\n",
        "\n",
        "            # Generate samples around the learned distribution\n",
        "            random_latent_vectors = np.random.normal(\n",
        "                loc=latent_mean,\n",
        "                scale=latent_std * 1.5,  # Add some variability\n",
        "                size=(num_samples, self.latent_dim)\n",
        "            )\n",
        "\n",
        "        # Decode to generate synthetic data\n",
        "        synthetic_data = self.decoder.predict(random_latent_vectors)\n",
        "\n",
        "        print(f\"Generated {len(synthetic_data)} synthetic samples\")\n",
        "        return synthetic_data\n",
        "\n",
        "    def create_balanced_dataset(self):\n",
        "        \"\"\"\n",
        "        Create balanced dataset using original + synthetic data\n",
        "        \"\"\"\n",
        "        print(\"Creating balanced dataset...\")\n",
        "\n",
        "        # Calculate how many synthetic samples needed\n",
        "        majority_count = len(self.dropout_sequences)\n",
        "        minority_count = len(self.no_dropout_sequences)\n",
        "        samples_needed = majority_count - minority_count\n",
        "\n",
        "        if samples_needed > 0:\n",
        "            # Generate synthetic data\n",
        "            synthetic_data = self.generate_synthetic_data(samples_needed)\n",
        "\n",
        "            # Combine original and synthetic data\n",
        "            balanced_X = np.vstack([\n",
        "                self.original_data['X'],  # Original data\n",
        "                synthetic_data  # Synthetic minority samples\n",
        "            ])\n",
        "\n",
        "            # Create corresponding labels\n",
        "            balanced_y = np.hstack([\n",
        "                self.original_data['y'],  # Original labels\n",
        "                np.zeros(samples_needed)  # Synthetic minority labels (0 = no dropout)\n",
        "            ])\n",
        "\n",
        "        else:\n",
        "            print(\"Dataset is already balanced or minority is larger\")\n",
        "            balanced_X = self.original_data['X']\n",
        "            balanced_y = self.original_data['y']\n",
        "\n",
        "        print(f\"Balanced dataset size: {len(balanced_X)}\")\n",
        "        print(f\"Class distribution after balancing: {np.unique(balanced_y, return_counts=True)}\")\n",
        "\n",
        "        self.augmented_data = {\n",
        "            'X': balanced_X,\n",
        "            'y': balanced_y,\n",
        "            'synthetic_samples': samples_needed if samples_needed > 0 else 0\n",
        "        }\n",
        "\n",
        "        return balanced_X, balanced_y\n",
        "\n",
        "    def build_lstm_model(self):\n",
        "        \"\"\"\n",
        "        Build LSTM model for dropout prediction\n",
        "        \"\"\"\n",
        "        print(\"Building LSTM model...\")\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            # First LSTM layer\n",
        "            layers.LSTM(64,\n",
        "                       return_sequences=True,\n",
        "                       input_shape=self.input_shape,\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2),\n",
        "\n",
        "            # Second LSTM layer\n",
        "            layers.LSTM(32,\n",
        "                       return_sequences=False,\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2),\n",
        "\n",
        "            # Dense layers\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(8, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        self.lstm_model = model\n",
        "        print(\"LSTM model built successfully!\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lstm(self, epochs=100, batch_size=64):\n",
        "        \"\"\"\n",
        "        Train LSTM model on balanced dataset\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        print(\"Training LSTM model...\")\n",
        "\n",
        "        if self.augmented_data is None:\n",
        "            print(\"Creating balanced dataset first...\")\n",
        "            self.create_balanced_dataset()\n",
        "\n",
        "        X = self.augmented_data['X']\n",
        "        y = self.augmented_data['y']\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10)\n",
        "\n",
        "        # Train model\n",
        "        lstm_history = self.lstm_model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['lstm'] = lstm_history.history\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_predictions = self.lstm_model.predict(X_test)\n",
        "        test_predictions_binary = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        self.test_metrics = {\n",
        "            'accuracy': accuracy_score(y_test, test_predictions_binary),\n",
        "            'precision': precision_score(y_test, test_predictions_binary),\n",
        "            'recall': recall_score(y_test, test_predictions_binary),\n",
        "            'f1_score': f1_score(y_test, test_predictions_binary)\n",
        "        }\n",
        "\n",
        "        print(\"\\nLSTM training completed!\")\n",
        "        print(\"\\nTest Set Performance:\")\n",
        "        for metric, value in self.test_metrics.items():\n",
        "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "        return lstm_history, X_test, y_test, test_predictions\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training history for both VAE and LSTM\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # VAE Loss\n",
        "        if 'vae' in self.history:\n",
        "            axes[0, 0].plot(self.history['vae']['loss'], label='Training Loss')\n",
        "            if 'val_loss' in self.history['vae']:\n",
        "                axes[0, 0].plot(self.history['vae']['val_loss'], label='Validation Loss')\n",
        "            axes[0, 0].set_title('VAE Training Loss')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True)\n",
        "\n",
        "        # LSTM Loss\n",
        "        if 'lstm' in self.history:\n",
        "            axes[0, 1].plot(self.history['lstm']['loss'], label='Training Loss')\n",
        "            axes[0, 1].plot(self.history['lstm']['val_loss'], label='Validation Loss')\n",
        "            axes[0, 1].set_title('LSTM Training Loss')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Loss')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "\n",
        "        # LSTM Accuracy\n",
        "        if 'lstm' in self.history:\n",
        "            axes[1, 0].plot(self.history['lstm']['accuracy'], label='Training Accuracy')\n",
        "            axes[1, 0].plot(self.history['lstm']['val_accuracy'], label='Validation Accuracy')\n",
        "            axes[1, 0].set_title('LSTM Training Accuracy')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Accuracy')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True)\n",
        "\n",
        "        # Model Performance Metrics\n",
        "        if hasattr(self, 'test_metrics'):\n",
        "            metrics_names = list(self.test_metrics.keys())\n",
        "            metrics_values = list(self.test_metrics.values())\n",
        "\n",
        "            axes[1, 1].bar(metrics_names, metrics_values)\n",
        "            axes[1, 1].set_title('Test Set Performance Metrics')\n",
        "            axes[1, 1].set_ylabel('Score')\n",
        "            axes[1, 1].set_ylim(0, 1)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for i, v in enumerate(metrics_values):\n",
        "                axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix\n",
        "        \"\"\"\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "        cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['No Dropout', 'Dropout'],\n",
        "                    yticklabels=['No Dropout', 'Dropout'])\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred_binary,\n",
        "                                  target_names=['No Dropout', 'Dropout']))\n",
        "\n",
        "    def predict_dropout(self, student_sequences):\n",
        "        \"\"\"\n",
        "        Predict dropout probability for new student data\n",
        "\n",
        "        Args:\n",
        "            student_sequences: Array of shape (n_students, 30, 7)\n",
        "\n",
        "        Returns:\n",
        "            Dropout probabilities\n",
        "        \"\"\"\n",
        "        if self.lstm_model is None:\n",
        "            raise ValueError(\"LSTM model not trained yet!\")\n",
        "\n",
        "        # Normalize the input sequences\n",
        "        sequences_flat = student_sequences.reshape(-1, self.sequence_length * self.features_per_day)\n",
        "        sequences_normalized = self.scaler.transform(sequences_flat)\n",
        "        sequences_reshaped = sequences_normalized.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.lstm_model.predict(sequences_reshaped)\n",
        "\n",
        "        return predictions.flatten()\n",
        "\n",
        "    def run_complete_pipeline(self, csv_file_path, vae_epochs=100, lstm_epochs=100):\n",
        "        \"\"\"\n",
        "        Run the complete VAE-LSTM pipeline\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file\n",
        "            vae_epochs (int): Epochs for VAE training\n",
        "            lstm_epochs (int): Epochs for LSTM training\n",
        "        \"\"\"\n",
        "        print(\"=\"*50)\n",
        "        print(\"STARTING VAE-LSTM DROPOUT PREDICTION PIPELINE\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Step 1: Load and preprocess data\n",
        "        X, y = self.load_and_preprocess_data(csv_file_path)\n",
        "\n",
        "        # Step 2: Build and train VAE\n",
        "        self.build_vae()\n",
        "        self.train_vae(epochs=vae_epochs)\n",
        "\n",
        "        # Step 3: Create balanced dataset\n",
        "        balanced_X, balanced_y = self.create_balanced_dataset()\n",
        "\n",
        "        # Step 4: Build and train LSTM\n",
        "        self.build_lstm_model()\n",
        "        lstm_history, X_test, y_test, test_predictions = self.train_lstm(epochs=lstm_epochs)\n",
        "\n",
        "        # Step 5: Visualize results\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"TRAINING COMPLETED - GENERATING VISUALIZATIONS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        self.plot_training_history()\n",
        "        self.plot_confusion_matrix(y_test, test_predictions)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return self.test_metrics\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the predictor\n",
        "    predictor = VAELSTMDropoutPredictor(\n",
        "        sequence_length=30,\n",
        "        features_per_day=7,\n",
        "        latent_dim=32\n",
        "    )\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    # Replace 'your_data.csv' with the actual path to your CSV file\n",
        "    csv_file_path = 'model1_210_features.csv'\n",
        "\n",
        "    try:\n",
        "        final_metrics = predictor.run_complete_pipeline(\n",
        "            csv_file_path=csv_file_path,\n",
        "            vae_epochs=50,  # Reduce for faster training\n",
        "            lstm_epochs=50\n",
        "        )\n",
        "\n",
        "        print(\"\\nFinal Model Performance:\")\n",
        "        for metric, value in final_metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find the file '{csv_file_path}'\")\n",
        "        print(\"Please make sure the CSV file is in the correct location.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    # Example of making predictions on new data\n",
        "    # Uncomment the following lines if you want to test predictions\n",
        "    \"\"\"\n",
        "    # Create some dummy test data (replace with real data)\n",
        "    test_sequences = np.random.rand(5, 30, 7)  # 5 students, 30 days, 7 features\n",
        "\n",
        "    # Make predictions\n",
        "    dropout_probabilities = predictor.predict_dropout(test_sequences)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i, prob in enumerate(dropout_probabilities):\n",
        "        risk_level = \"High\" if prob > 0.7 else \"Medium\" if prob > 0.3 else \"Low\"\n",
        "        print(f\"Student {i+1}: {prob:.3f} probability ({risk_level} risk)\")\n",
        "    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMJBaXcYNCLs",
        "outputId": "c7a53656-073b-4980-e5cc-8a1e9505631b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "STARTING VAE-LSTM DROPOUT PREDICTION PIPELINE\n",
            "==================================================\n",
            "Loading and preprocessing data...\n",
            "Loaded 120542 samples with 214 columns\n",
            "Class distribution: {np.int64(0): np.int64(24961), np.int64(1): np.int64(95581)}\n",
            "Dropout samples: 95581\n",
            "No dropout samples: 24961\n",
            "Imbalance ratio: 3.83\n",
            "Building VAE model...\n",
            "VAE model built successfully!\n",
            "Training VAE on minority class (no dropout)...\n",
            "Epoch 1/50\n",
            "An error occurred: Invalid reduction dimension 2 for input with 2 dimensions. for '{{node Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](Mean, Sum/reduction_indices)' with input shapes: [32,30], [2] and with computed input tensors: input[1] = <1 2>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, optimizers, losses\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class VAELSTMDropoutPredictor:\n",
        "    def __init__(self, sequence_length=30, features_per_day=7, latent_dim=32):\n",
        "        \"\"\"\n",
        "        Initialize the VAE-LSTM Dropout Predictor\n",
        "\n",
        "        Args:\n",
        "            sequence_length (int): Number of days in sequence (30)\n",
        "            features_per_day (int): Number of features per day (7)\n",
        "            latent_dim (int): Latent dimension for VAE\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.features_per_day = features_per_day\n",
        "        self.latent_dim = latent_dim\n",
        "        self.input_shape = (sequence_length, features_per_day)\n",
        "\n",
        "        # Models\n",
        "        self.vae = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.lstm_model = None\n",
        "\n",
        "        # Data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.original_data = None\n",
        "        self.augmented_data = None\n",
        "        self.history = {}\n",
        "\n",
        "    def load_and_preprocess_data(self, csv_file_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess the student activity data\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file\n",
        "        \"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
        "\n",
        "        # Extract feature columns (day_1_access to day_30_video)\n",
        "        feature_columns = []\n",
        "        for day in range(1, 31):  # 30 days\n",
        "            feature_columns.extend([\n",
        "                f'day_{day}_access',\n",
        "                f'day_{day}_problem',\n",
        "                f'day_{day}_wiki',\n",
        "                f'day_{day}_discussion',\n",
        "                f'day_{day}_navigate',\n",
        "                f'day_{day}_page_close',\n",
        "                f'day_{day}_video'\n",
        "            ])\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = df[feature_columns].values\n",
        "        y = df['dropout'].values\n",
        "\n",
        "        # Reshape data for LSTM (samples, timesteps, features)\n",
        "        X_sequences = X.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Normalize features\n",
        "        X_flat = X_sequences.reshape(-1, self.sequence_length * self.features_per_day)\n",
        "        X_normalized = self.scaler.fit_transform(X_flat)\n",
        "        X_sequences = X_normalized.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Analyze class distribution\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        class_distribution = dict(zip(unique, counts))\n",
        "        print(f\"Class distribution: {class_distribution}\")\n",
        "\n",
        "        # Separate by class\n",
        "        dropout_indices = np.where(y == 1)[0]\n",
        "        no_dropout_indices = np.where(y == 0)[0]\n",
        "\n",
        "        self.dropout_sequences = X_sequences[dropout_indices]\n",
        "        self.no_dropout_sequences = X_sequences[no_dropout_indices]\n",
        "\n",
        "        print(f\"Dropout samples: {len(self.dropout_sequences)}\")\n",
        "        print(f\"No dropout samples: {len(self.no_dropout_sequences)}\")\n",
        "        print(f\"Imbalance ratio: {len(self.dropout_sequences) / len(self.no_dropout_sequences):.2f}\")\n",
        "\n",
        "        self.original_data = {\n",
        "            'X': X_sequences,\n",
        "            'y': y,\n",
        "            'class_distribution': class_distribution\n",
        "        }\n",
        "\n",
        "        return X_sequences, y\n",
        "\n",
        "    def build_vae(self):\n",
        "        \"\"\"\n",
        "        Build Variational Autoencoder for data augmentation\n",
        "        \"\"\"\n",
        "        print(\"Building VAE model...\")\n",
        "\n",
        "        # Custom sampling layer\n",
        "        class Sampling(layers.Layer):\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var = inputs\n",
        "                batch = tf.shape(z_mean)[0]\n",
        "                dim = tf.shape(z_mean)[1]\n",
        "                epsilon = tf.random.normal(shape=(batch, dim))\n",
        "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        # Encoder\n",
        "        encoder_inputs = layers.Input(shape=self.input_shape, name='encoder_input')\n",
        "        x = layers.Flatten()(encoder_inputs)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "        # Latent space\n",
        "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)\n",
        "\n",
        "        # Sampling\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "        # Create encoder model\n",
        "        self.encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "        # Decoder\n",
        "        decoder_inputs = layers.Input(shape=(self.latent_dim,), name='decoder_input')\n",
        "        x = layers.Dense(128, activation='relu')(decoder_inputs)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dense(self.sequence_length * self.features_per_day, activation='sigmoid')(x)\n",
        "        decoder_outputs = layers.Reshape(self.input_shape)(x)\n",
        "\n",
        "        # Create decoder model\n",
        "        self.decoder = Model(decoder_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "        # VAE model\n",
        "        vae_outputs = self.decoder(self.encoder(encoder_inputs)[2])\n",
        "        self.vae = Model(encoder_inputs, vae_outputs, name='vae')\n",
        "\n",
        "        # Custom VAE loss using subclassing\n",
        "        class VAE(Model):\n",
        "            def __init__(self, encoder, decoder, **kwargs):\n",
        "                super(VAE, self).__init__(**kwargs)\n",
        "                self.encoder = encoder\n",
        "                self.decoder = decoder\n",
        "                self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "                self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "                self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "            @property\n",
        "            def metrics(self):\n",
        "                return [\n",
        "                    self.total_loss_tracker,\n",
        "                    self.reconstruction_loss_tracker,\n",
        "                    self.kl_loss_tracker,\n",
        "                ]\n",
        "\n",
        "            def train_step(self, data):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    z_mean, z_log_var, z = self.encoder(data)\n",
        "                    reconstruction = self.decoder(z)\n",
        "\n",
        "                    # Flatten both data and reconstruction for loss calculation\n",
        "                    data_flat = tf.reshape(data, [tf.shape(data)[0], -1])\n",
        "                    reconstruction_flat = tf.reshape(reconstruction, [tf.shape(reconstruction)[0], -1])\n",
        "\n",
        "                    # Reconstruction loss - MSE loss\n",
        "                    reconstruction_loss = tf.reduce_mean(\n",
        "                        tf.reduce_sum(\n",
        "                            tf.square(data_flat - reconstruction_flat), axis=1\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    # KL divergence loss\n",
        "                    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "                    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "                    # Total loss with weighting\n",
        "                    total_loss = reconstruction_loss + 0.5 * kl_loss\n",
        "\n",
        "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "                self.total_loss_tracker.update_state(total_loss)\n",
        "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "                self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "                return {\n",
        "                    \"loss\": self.total_loss_tracker.result(),\n",
        "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                    \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "                }\n",
        "\n",
        "            def call(self, inputs):\n",
        "                z_mean, z_log_var, z = self.encoder(inputs)\n",
        "                return self.decoder(z)\n",
        "\n",
        "        # Create the custom VAE model\n",
        "        self.vae = VAE(self.encoder, self.decoder)\n",
        "        self.vae.compile(optimizer='adam')\n",
        "\n",
        "        print(\"VAE model built successfully!\")\n",
        "        return self.vae\n",
        "\n",
        "    def train_vae(self, epochs=100, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the VAE on minority class data\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        print(\"Training VAE on minority class (no dropout)...\")\n",
        "\n",
        "        # Use minority class for VAE training\n",
        "        minority_data = self.no_dropout_sequences\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        )\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "            monitor='loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Train VAE\n",
        "        vae_history = self.vae.fit(\n",
        "            minority_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['vae'] = vae_history.history\n",
        "        print(\"VAE training completed!\")\n",
        "\n",
        "        return vae_history\n",
        "\n",
        "    def generate_synthetic_data(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate synthetic data using trained VAE\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): Number of synthetic samples to generate\n",
        "        \"\"\"\n",
        "        print(f\"Generating {num_samples} synthetic samples...\")\n",
        "\n",
        "        # Sample from latent space\n",
        "        random_latent_vectors = np.random.normal(size=(num_samples, self.latent_dim))\n",
        "\n",
        "        # Add some structure by sampling around the learned distribution\n",
        "        if len(self.no_dropout_sequences) > 0:\n",
        "            # Encode some real samples to get latent distribution\n",
        "            z_mean, z_log_var, _ = self.encoder.predict(self.no_dropout_sequences[:100])\n",
        "\n",
        "            # Sample around the mean of the latent distribution\n",
        "            latent_mean = np.mean(z_mean, axis=0)\n",
        "            latent_std = np.std(z_mean, axis=0)\n",
        "\n",
        "            # Generate samples around the learned distribution\n",
        "            random_latent_vectors = np.random.normal(\n",
        "                loc=latent_mean,\n",
        "                scale=latent_std * 1.5,  # Add some variability\n",
        "                size=(num_samples, self.latent_dim)\n",
        "            )\n",
        "\n",
        "        # Decode to generate synthetic data\n",
        "        synthetic_data = self.decoder.predict(random_latent_vectors)\n",
        "\n",
        "        print(f\"Generated {len(synthetic_data)} synthetic samples\")\n",
        "        return synthetic_data\n",
        "\n",
        "    def create_balanced_dataset(self):\n",
        "        \"\"\"\n",
        "        Create balanced dataset using original + synthetic data\n",
        "        \"\"\"\n",
        "        print(\"Creating balanced dataset...\")\n",
        "\n",
        "        # Calculate how many synthetic samples needed\n",
        "        majority_count = len(self.dropout_sequences)\n",
        "        minority_count = len(self.no_dropout_sequences)\n",
        "        samples_needed = majority_count - minority_count\n",
        "\n",
        "        if samples_needed > 0:\n",
        "            # Generate synthetic data\n",
        "            synthetic_data = self.generate_synthetic_data(samples_needed)\n",
        "\n",
        "            # Combine original and synthetic data\n",
        "            balanced_X = np.vstack([\n",
        "                self.original_data['X'],  # Original data\n",
        "                synthetic_data  # Synthetic minority samples\n",
        "            ])\n",
        "\n",
        "            # Create corresponding labels\n",
        "            balanced_y = np.hstack([\n",
        "                self.original_data['y'],  # Original labels\n",
        "                np.zeros(samples_needed)  # Synthetic minority labels (0 = no dropout)\n",
        "            ])\n",
        "\n",
        "        else:\n",
        "            print(\"Dataset is already balanced or minority is larger\")\n",
        "            balanced_X = self.original_data['X']\n",
        "            balanced_y = self.original_data['y']\n",
        "\n",
        "        print(f\"Balanced dataset size: {len(balanced_X)}\")\n",
        "        print(f\"Class distribution after balancing: {np.unique(balanced_y, return_counts=True)}\")\n",
        "\n",
        "        self.augmented_data = {\n",
        "            'X': balanced_X,\n",
        "            'y': balanced_y,\n",
        "            'synthetic_samples': samples_needed if samples_needed > 0 else 0\n",
        "        }\n",
        "\n",
        "        return balanced_X, balanced_y\n",
        "\n",
        "    def build_lstm_model(self):\n",
        "        \"\"\"\n",
        "        Build LSTM model for dropout prediction\n",
        "        \"\"\"\n",
        "        print(\"Building LSTM model...\")\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            # First LSTM layer\n",
        "            layers.LSTM(64,\n",
        "                       return_sequences=True,\n",
        "                       input_shape=self.input_shape,\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2),\n",
        "\n",
        "            # Second LSTM layer\n",
        "            layers.LSTM(32,\n",
        "                       return_sequences=False,\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2),\n",
        "\n",
        "            # Dense layers\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(8, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        self.lstm_model = model\n",
        "        print(\"LSTM model built successfully!\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lstm(self, epochs=100, batch_size=64):\n",
        "        \"\"\"\n",
        "        Train LSTM model on balanced dataset\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        print(\"Training LSTM model...\")\n",
        "\n",
        "        if self.augmented_data is None:\n",
        "            print(\"Creating balanced dataset first...\")\n",
        "            self.create_balanced_dataset()\n",
        "\n",
        "        X = self.augmented_data['X']\n",
        "        y = self.augmented_data['y']\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(factor=0.5, patience=10)\n",
        "\n",
        "        # Train model\n",
        "        lstm_history = self.lstm_model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history['lstm'] = lstm_history.history\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_predictions = self.lstm_model.predict(X_test)\n",
        "        test_predictions_binary = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        self.test_metrics = {\n",
        "            'accuracy': accuracy_score(y_test, test_predictions_binary),\n",
        "            'precision': precision_score(y_test, test_predictions_binary),\n",
        "            'recall': recall_score(y_test, test_predictions_binary),\n",
        "            'f1_score': f1_score(y_test, test_predictions_binary)\n",
        "        }\n",
        "\n",
        "        print(\"\\nLSTM training completed!\")\n",
        "        print(\"\\nTest Set Performance:\")\n",
        "        for metric, value in self.test_metrics.items():\n",
        "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "        return lstm_history, X_test, y_test, test_predictions\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training history for both VAE and LSTM\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # VAE Loss\n",
        "        if 'vae' in self.history:\n",
        "            axes[0, 0].plot(self.history['vae']['loss'], label='Training Loss')\n",
        "            if 'val_loss' in self.history['vae']:\n",
        "                axes[0, 0].plot(self.history['vae']['val_loss'], label='Validation Loss')\n",
        "            axes[0, 0].set_title('VAE Training Loss')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True)\n",
        "\n",
        "        # LSTM Loss\n",
        "        if 'lstm' in self.history:\n",
        "            axes[0, 1].plot(self.history['lstm']['loss'], label='Training Loss')\n",
        "            axes[0, 1].plot(self.history['lstm']['val_loss'], label='Validation Loss')\n",
        "            axes[0, 1].set_title('LSTM Training Loss')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Loss')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "\n",
        "        # LSTM Accuracy\n",
        "        if 'lstm' in self.history:\n",
        "            axes[1, 0].plot(self.history['lstm']['accuracy'], label='Training Accuracy')\n",
        "            axes[1, 0].plot(self.history['lstm']['val_accuracy'], label='Validation Accuracy')\n",
        "            axes[1, 0].set_title('LSTM Training Accuracy')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Accuracy')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True)\n",
        "\n",
        "        # Model Performance Metrics\n",
        "        if hasattr(self, 'test_metrics'):\n",
        "            metrics_names = list(self.test_metrics.keys())\n",
        "            metrics_values = list(self.test_metrics.values())\n",
        "\n",
        "            axes[1, 1].bar(metrics_names, metrics_values)\n",
        "            axes[1, 1].set_title('Test Set Performance Metrics')\n",
        "            axes[1, 1].set_ylabel('Score')\n",
        "            axes[1, 1].set_ylim(0, 1)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for i, v in enumerate(metrics_values):\n",
        "                axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix\n",
        "        \"\"\"\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "        cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['No Dropout', 'Dropout'],\n",
        "                    yticklabels=['No Dropout', 'Dropout'])\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred_binary,\n",
        "                                  target_names=['No Dropout', 'Dropout']))\n",
        "\n",
        "    def predict_dropout(self, student_sequences):\n",
        "        \"\"\"\n",
        "        Predict dropout probability for new student data\n",
        "\n",
        "        Args:\n",
        "            student_sequences: Array of shape (n_students, 30, 7)\n",
        "\n",
        "        Returns:\n",
        "            Dropout probabilities\n",
        "        \"\"\"\n",
        "        if self.lstm_model is None:\n",
        "            raise ValueError(\"LSTM model not trained yet!\")\n",
        "\n",
        "        # Normalize the input sequences\n",
        "        sequences_flat = student_sequences.reshape(-1, self.sequence_length * self.features_per_day)\n",
        "        sequences_normalized = self.scaler.transform(sequences_flat)\n",
        "        sequences_reshaped = sequences_normalized.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.lstm_model.predict(sequences_reshaped)\n",
        "\n",
        "        return predictions.flatten()\n",
        "\n",
        "    def run_complete_pipeline(self, csv_file_path, vae_epochs=100, lstm_epochs=100):\n",
        "        \"\"\"\n",
        "        Run the complete VAE-LSTM pipeline\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file\n",
        "            vae_epochs (int): Epochs for VAE training\n",
        "            lstm_epochs (int): Epochs for LSTM training\n",
        "        \"\"\"\n",
        "        print(\"=\"*50)\n",
        "        print(\"STARTING VAE-LSTM DROPOUT PREDICTION PIPELINE\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Step 1: Load and preprocess data\n",
        "        X, y = self.load_and_preprocess_data(csv_file_path)\n",
        "\n",
        "        # Step 2: Build and train VAE\n",
        "        self.build_vae()\n",
        "        self.train_vae(epochs=vae_epochs)\n",
        "\n",
        "        # Step 3: Create balanced dataset\n",
        "        balanced_X, balanced_y = self.create_balanced_dataset()\n",
        "\n",
        "        # Step 4: Build and train LSTM\n",
        "        self.build_lstm_model()\n",
        "        lstm_history, X_test, y_test, test_predictions = self.train_lstm(epochs=lstm_epochs)\n",
        "\n",
        "        # Step 5: Visualize results\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"TRAINING COMPLETED - GENERATING VISUALIZATIONS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        self.plot_training_history()\n",
        "        self.plot_confusion_matrix(y_test, test_predictions)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return self.test_metrics\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the predictor\n",
        "    predictor = VAELSTMDropoutPredictor(\n",
        "        sequence_length=30,\n",
        "        features_per_day=7,\n",
        "        latent_dim=32\n",
        "    )\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    # Replace 'your_data.csv' with the actual path to your CSV file\n",
        "    csv_file_path = 'model1_210_features.csv'\n",
        "\n",
        "    try:\n",
        "        final_metrics = predictor.run_complete_pipeline(\n",
        "            csv_file_path=csv_file_path,\n",
        "            vae_epochs=50,  # Reduce for faster training\n",
        "            lstm_epochs=50\n",
        "        )\n",
        "\n",
        "        print(\"\\nFinal Model Performance:\")\n",
        "        for metric, value in final_metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find the file '{csv_file_path}'\")\n",
        "        print(\"Please make sure the CSV file is in the correct location.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    # Example of making predictions on new data\n",
        "    # Uncomment the following lines if you want to test predictions\n",
        "    \"\"\"\n",
        "    # Create some dummy test data (replace with real data)\n",
        "    test_sequences = np.random.rand(5, 30, 7)  # 5 students, 30 days, 7 features\n",
        "\n",
        "    # Make predictions\n",
        "    dropout_probabilities = predictor.predict_dropout(test_sequences)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i, prob in enumerate(dropout_probabilities):\n",
        "        risk_level = \"High\" if prob > 0.7 else \"Medium\" if prob > 0.3 else \"Low\"\n",
        "        print(f\"Student {i+1}: {prob:.3f} probability ({risk_level} risk)\")\n",
        "    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57_-_cpSN912",
        "outputId": "16531688-0ba3-44df-e766-50fc7838d227"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "STARTING VAE-LSTM DROPOUT PREDICTION PIPELINE\n",
            "==================================================\n",
            "Loading and preprocessing data...\n",
            "Loaded 120542 samples with 214 columns\n",
            "Class distribution: {np.int64(0): np.int64(24961), np.int64(1): np.int64(95581)}\n",
            "Dropout samples: 95581\n",
            "No dropout samples: 24961\n",
            "Imbalance ratio: 3.83\n",
            "Building VAE model...\n",
            "VAE model built successfully!\n",
            "Training VAE on minority class (no dropout)...\n",
            "Epoch 1/50\n",
            "\u001b[1m623/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - kl_loss: 6885455360.0000 - loss: 3442727936.0000 - reconstruction_loss: 929.9529An error occurred: No loss to compute. Provide a `loss` argument in `compile()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.stats import uniform, randint\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class AttentionLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Attention Layer for LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, units=128, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Attention weights\n",
        "        self.W_a = self.add_weight(\n",
        "            name='W_a',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.U_a = self.add_weight(\n",
        "            name='U_a',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.V_a = self.add_weight(\n",
        "            name='V_a',\n",
        "            shape=(self.units, 1),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, time_steps, features)\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Calculate attention scores\n",
        "        # For each time step, compute attention with all other time steps\n",
        "        scores = []\n",
        "        for i in range(seq_len):\n",
        "            # Current hidden state\n",
        "            h_i = inputs[:, i:i+1, :]  # (batch_size, 1, features)\n",
        "            h_i_repeated = tf.repeat(h_i, seq_len, axis=1)  # (batch_size, seq_len, features)\n",
        "\n",
        "            # Attention mechanism\n",
        "            score = tf.nn.tanh(\n",
        "                tf.matmul(inputs, self.W_a) + tf.matmul(h_i_repeated, self.U_a)\n",
        "            )  # (batch_size, seq_len, units)\n",
        "\n",
        "            score = tf.matmul(score, self.V_a)  # (batch_size, seq_len, 1)\n",
        "            scores.append(score)\n",
        "\n",
        "        # Concatenate all scores\n",
        "        attention_scores = tf.concat(scores, axis=-1)  # (batch_size, seq_len, seq_len)\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        context_vector = tf.matmul(attention_weights, inputs, transpose_a=True)  # (batch_size, seq_len, features)\n",
        "\n",
        "        # Global attention pooling\n",
        "        global_attention = tf.reduce_mean(context_vector, axis=1)  # (batch_size, features)\n",
        "\n",
        "        return global_attention, attention_weights\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [(input_shape[0], input_shape[2]), (input_shape[0], input_shape[1], input_shape[1])]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(AttentionLayer, self).get_config()\n",
        "        config.update({'units': self.units})\n",
        "        return config\n",
        "\n",
        "class SelfAttentionLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention Layer (Simplified Transformer-style)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=64, num_heads=8, **kwargs):\n",
        "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.wq = layers.Dense(self.d_model)\n",
        "        self.wk = layers.Dense(self.d_model)\n",
        "        self.wv = layers.Dense(self.d_model)\n",
        "        self.dense = layers.Dense(input_shape[-1])\n",
        "        super(SelfAttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        \"\"\"Calculate the attention weights and apply to values\"\"\"\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "        # Scale matmul_qk\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        q = self.wq(inputs)\n",
        "        k = self.wk(inputs)\n",
        "        v = self.wv(inputs)\n",
        "\n",
        "        # Apply attention\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.dense(attention_output)\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        output = layers.LayerNormalization()(output + inputs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SelfAttentionLayer, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class AttentionLSTMClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    LSTM with Attention Mechanism wrapped as sklearn estimator\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 lstm_units=64,\n",
        "                 attention_units=128,\n",
        "                 dropout_rate=0.2,\n",
        "                 recurrent_dropout=0.2,\n",
        "                 dense_units=32,\n",
        "                 learning_rate=0.001,\n",
        "                 batch_size=32,\n",
        "                 epochs=50,\n",
        "                 attention_type='custom',\n",
        "                 use_bidirectional=True,\n",
        "                 num_lstm_layers=2):\n",
        "\n",
        "        self.lstm_units = lstm_units\n",
        "        self.attention_units = attention_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.recurrent_dropout = recurrent_dropout\n",
        "        self.dense_units = dense_units\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.attention_type = attention_type\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.num_lstm_layers = num_lstm_layers\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.history = None\n",
        "\n",
        "    def _build_model(self, input_shape):\n",
        "        \"\"\"Build the Attention-LSTM model\"\"\"\n",
        "        inputs = layers.Input(shape=input_shape)\n",
        "        x = inputs\n",
        "\n",
        "        # LSTM layers\n",
        "        for i in range(self.num_lstm_layers):\n",
        "            return_sequences = True if i < self.num_lstm_layers - 1 or self.attention_type != 'none' else False\n",
        "\n",
        "            lstm_layer = layers.LSTM(\n",
        "                self.lstm_units,\n",
        "                return_sequences=return_sequences,\n",
        "                dropout=self.dropout_rate,\n",
        "                recurrent_dropout=self.recurrent_dropout,\n",
        "                name=f'lstm_{i+1}'\n",
        "            )\n",
        "\n",
        "            if self.use_bidirectional:\n",
        "                x = layers.Bidirectional(lstm_layer, name=f'bidirectional_{i+1}')(x)\n",
        "            else:\n",
        "                x = lstm_layer(x)\n",
        "\n",
        "        # Attention mechanism\n",
        "        if self.attention_type == 'custom':\n",
        "            attention_output, attention_weights = AttentionLayer(\n",
        "                units=self.attention_units,\n",
        "                name='attention_layer'\n",
        "            )(x)\n",
        "            x = attention_output\n",
        "\n",
        "        elif self.attention_type == 'self':\n",
        "            x = SelfAttentionLayer(\n",
        "                d_model=self.lstm_units * (2 if self.use_bidirectional else 1),\n",
        "                name='self_attention'\n",
        "            )(x)\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        elif self.attention_type == 'global':\n",
        "            # Global attention pooling\n",
        "            attention_weights = layers.Dense(1, activation='softmax')(x)\n",
        "            x = tf.reduce_sum(x * attention_weights, axis=1)\n",
        "\n",
        "        elif self.attention_type == 'none':\n",
        "            # No attention, just use last output\n",
        "            pass\n",
        "\n",
        "        # Dense layers\n",
        "        x = layers.Dense(self.dense_units, activation='relu', name='dense_1')(x)\n",
        "        x = layers.Dropout(self.dropout_rate)(x)\n",
        "        x = layers.Dense(self.dense_units // 2, activation='relu', name='dense_2')(x)\n",
        "        x = layers.Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "        model = Model(inputs, outputs, name='attention_lstm_classifier')\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=self.learning_rate),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model\"\"\"\n",
        "        # Handle input shape\n",
        "        if len(X.shape) == 2:\n",
        "            # Reshape to (samples, timesteps, features)\n",
        "            X = X.reshape(X.shape[0], 30, 7)  # 30 days, 7 features per day\n",
        "\n",
        "        # Scale features\n",
        "        original_shape = X.shape\n",
        "        X_flat = X.reshape(-1, X.shape[-1])\n",
        "        X_scaled = self.scaler.fit_transform(X_flat)\n",
        "        X = X_scaled.reshape(original_shape)\n",
        "\n",
        "        # Build model\n",
        "        self.model = self._build_model(X.shape[1:])\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=0\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                verbose=0\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        self.history = self.model.fit(\n",
        "            X, y,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=self.epochs,\n",
        "            validation_split=0.2,\n",
        "            callbacks=callbacks,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        if len(X.shape) == 2:\n",
        "            X = X.reshape(X.shape[0], 30, 7)\n",
        "\n",
        "        # Scale features\n",
        "        original_shape = X.shape\n",
        "        X_flat = X.reshape(-1, X.shape[-1])\n",
        "        X_scaled = self.scaler.transform(X_flat)\n",
        "        X = X_scaled.reshape(original_shape)\n",
        "\n",
        "        predictions = self.model.predict(X, verbose=0)\n",
        "        return (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities\"\"\"\n",
        "        if len(X.shape) == 2:\n",
        "            X = X.reshape(X.shape[0], 30, 7)\n",
        "\n",
        "        # Scale features\n",
        "        original_shape = X.shape\n",
        "        X_flat = X.reshape(-1, X.shape[-1])\n",
        "        X_scaled = self.scaler.transform(X_flat)\n",
        "        X = X_scaled.reshape(original_shape)\n",
        "\n",
        "        predictions = self.model.predict(X, verbose=0)\n",
        "        return np.column_stack([1 - predictions.flatten(), predictions.flatten()])\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Score the model (accuracy)\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "class StudentDropoutPredictor:\n",
        "    \"\"\"\n",
        "    Main class for student dropout prediction with Attention-LSTM and RandomizedSearchCV\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length=30, features_per_day=7):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.features_per_day = features_per_day\n",
        "        self.input_shape = (sequence_length, features_per_day)\n",
        "\n",
        "        # Models and results\n",
        "        self.best_model = None\n",
        "        self.randomized_search = None\n",
        "        self.results = {}\n",
        "\n",
        "    def load_and_preprocess_data(self, csv_file_path):\n",
        "        \"\"\"Load and preprocess the student activity data\"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
        "\n",
        "        # Extract feature columns\n",
        "        feature_columns = []\n",
        "        for day in range(1, 31):\n",
        "            feature_columns.extend([\n",
        "                f'day_{day}_access',\n",
        "                f'day_{day}_problem',\n",
        "                f'day_{day}_wiki',\n",
        "                f'day_{day}_discussion',\n",
        "                f'day_{day}_navigate',\n",
        "                f'day_{day}_page_close',\n",
        "                f'day_{day}_video'\n",
        "            ])\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = df[feature_columns].values\n",
        "        y = df['dropout'].values\n",
        "\n",
        "        # Reshape for time series\n",
        "        X_sequences = X.reshape(-1, self.sequence_length, self.features_per_day)\n",
        "\n",
        "        # Analyze class distribution\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        class_distribution = dict(zip(unique, counts))\n",
        "        print(f\"Class distribution: {class_distribution}\")\n",
        "        print(f\"Imbalance ratio: {counts[1] / counts[0]:.2f}\")\n",
        "\n",
        "        self.original_data = {\n",
        "            'X': X_sequences,\n",
        "            'y': y,\n",
        "            'class_distribution': class_distribution\n",
        "        }\n",
        "\n",
        "        return X_sequences, y\n",
        "\n",
        "    def define_hyperparameter_space(self):\n",
        "        \"\"\"Define hyperparameter search space\"\"\"\n",
        "        param_distributions = {\n",
        "            'lstm_units': randint(32, 128),\n",
        "            'attention_units': randint(64, 256),\n",
        "            'dropout_rate': uniform(0.1, 0.4),\n",
        "            'recurrent_dropout': uniform(0.1, 0.3),\n",
        "            'dense_units': randint(16, 64),\n",
        "            'learning_rate': uniform(0.0001, 0.01),\n",
        "            'batch_size': [16, 32, 64, 128],\n",
        "            'attention_type': ['custom', 'self', 'global', 'none'],\n",
        "            'use_bidirectional': [True, False],\n",
        "            'num_lstm_layers': randint(1, 3)\n",
        "        }\n",
        "\n",
        "        return param_distributions\n",
        "\n",
        "    def run_randomized_search(self, X, y, n_iter=50, cv_folds=3, n_jobs=-1):\n",
        "        \"\"\"Run RandomizedSearchCV for hyperparameter optimization\"\"\"\n",
        "        print(f\"Starting RandomizedSearchCV with {n_iter} iterations...\")\n",
        "\n",
        "        # Create base estimator\n",
        "        base_estimator = AttentionLSTMClassifier(epochs=30)  # Reduced epochs for faster search\n",
        "\n",
        "        # Define hyperparameter space\n",
        "        param_distributions = self.define_hyperparameter_space()\n",
        "\n",
        "        # Create stratified k-fold\n",
        "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        # Create RandomizedSearchCV\n",
        "        self.randomized_search = RandomizedSearchCV(\n",
        "            estimator=base_estimator,\n",
        "            param_distributions=param_distributions,\n",
        "            n_iter=n_iter,\n",
        "            cv=cv,\n",
        "            scoring='f1',  # Using F1 score for imbalanced dataset\n",
        "            n_jobs=n_jobs,\n",
        "            random_state=42,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Fit the search\n",
        "        print(\"Fitting RandomizedSearchCV...\")\n",
        "        self.randomized_search.fit(X, y)\n",
        "\n",
        "        print(\"RandomizedSearchCV completed!\")\n",
        "        print(f\"Best score: {self.randomized_search.best_score_:.4f}\")\n",
        "        print(f\"Best parameters: {self.randomized_search.best_params_}\")\n",
        "\n",
        "        return self.randomized_search\n",
        "\n",
        "    def train_best_model(self, X, y, test_size=0.2):\n",
        "        \"\"\"Train the best model found by RandomizedSearchCV\"\"\"\n",
        "        print(\"Training best model with full epochs...\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Get best parameters and increase epochs for final training\n",
        "        best_params = self.randomized_search.best_params_.copy()\n",
        "        best_params['epochs'] = 100  # Full training\n",
        "\n",
        "        # Create and train best model\n",
        "        self.best_model = AttentionLSTMClassifier(**best_params)\n",
        "        self.best_model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        y_pred = self.best_model.predict(X_test)\n",
        "        y_pred_proba = self.best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        self.results = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred),\n",
        "            'recall': recall_score(y_test, y_pred),\n",
        "            'f1_score': f1_score(y_test, y_pred),\n",
        "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
        "            'best_params': best_params,\n",
        "            'y_test': y_test,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "\n",
        "        print(\"\\nBest Model Performance:\")\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
        "            print(f\"{metric.capitalize()}: {self.results[metric]:.4f}\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def plot_hyperparameter_analysis(self):\n",
        "        \"\"\"Plot hyperparameter analysis from RandomizedSearchCV\"\"\"\n",
        "        if self.randomized_search is None:\n",
        "            print(\"No RandomizedSearchCV results to plot\")\n",
        "            return\n",
        "\n",
        "        # Convert results to DataFrame\n",
        "        results_df = pd.DataFrame(self.randomized_search.cv_results_)\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Plot 1: Score distribution\n",
        "        axes[0, 0].hist(results_df['mean_test_score'], bins=20, alpha=0.7, color='skyblue')\n",
        "        axes[0, 0].axvline(self.randomized_search.best_score_, color='red', linestyle='--',\n",
        "                          label=f'Best Score: {self.randomized_search.best_score_:.4f}')\n",
        "        axes[0, 0].set_title('Cross-Validation Score Distribution')\n",
        "        axes[0, 0].set_xlabel('F1 Score')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "        # Plot 2: LSTM units vs Score\n",
        "        axes[0, 1].scatter(results_df['param_lstm_units'], results_df['mean_test_score'], alpha=0.6)\n",
        "        axes[0, 1].set_title('LSTM Units vs F1 Score')\n",
        "        axes[0, 1].set_xlabel('LSTM Units')\n",
        "        axes[0, 1].set_ylabel('F1 Score')\n",
        "\n",
        "        # Plot 3: Learning rate vs Score\n",
        "        axes[0, 2].scatter(results_df['param_learning_rate'], results_df['mean_test_score'], alpha=0.6)\n",
        "        axes[0, 2].set_title('Learning Rate vs F1 Score')\n",
        "        axes[0, 2].set_xlabel('Learning Rate')\n",
        "        axes[0, 2].set_ylabel('F1 Score')\n",
        "        axes[0, 2].set_xscale('log')\n",
        "\n",
        "        # Plot 4: Attention type performance\n",
        "        attention_performance = results_df.groupby('param_attention_type')['mean_test_score'].mean().sort_values()\n",
        "        axes[1, 0].bar(attention_performance.index, attention_performance.values, color='lightcoral')\n",
        "        axes[1, 0].set_title('Attention Type Performance')\n",
        "        axes[1, 0].set_xlabel('Attention Type')\n",
        "        axes[1, 0].set_ylabel('Mean F1 Score')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Plot 5: Bidirectional vs Unidirectional\n",
        "        bidirectional_performance = results_df.groupby('param_use_bidirectional')['mean_test_score'].mean()\n",
        "        axes[1, 1].bar(['Unidirectional', 'Bidirectional'],\n",
        "                      [bidirectional_performance[False], bidirectional_performance[True]],\n",
        "                      color=['lightblue', 'lightgreen'])\n",
        "        axes[1, 1].set_title('Bidirectional vs Unidirectional LSTM')\n",
        "        axes[1, 1].set_ylabel('Mean F1 Score')\n",
        "\n",
        "        # Plot 6: Top 10 parameter combinations\n",
        "        top_10 = results_df.nlargest(10, 'mean_test_score')\n",
        "        axes[1, 2].barh(range(10), top_10['mean_test_score'].values[::-1])\n",
        "        axes[1, 2].set_title('Top 10 Parameter Combinations')\n",
        "        axes[1, 2].set_xlabel('F1 Score')\n",
        "        axes[1, 2].set_ylabel('Rank')\n",
        "        axes[1, 2].set_yticks(range(10))\n",
        "        axes[1, 2].set_yticklabels([f'#{i+1}' for i in range(10)])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_model_performance(self):\n",
        "        \"\"\"Plot model performance metrics and curves\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No model results to plot\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Plot 1: Performance metrics\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "        values = [self.results[metric] for metric in metrics]\n",
        "\n",
        "        bars = axes[0, 0].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
        "        axes[0, 0].set_title('Model Performance Metrics')\n",
        "        axes[0, 0].set_ylabel('Score')\n",
        "        axes[0, 0].set_ylim(0, 1)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Plot 2: Confusion Matrix\n",
        "        cm = confusion_matrix(self.results['y_test'], self.results['y_pred'])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
        "                   xticklabels=['No Dropout', 'Dropout'],\n",
        "                   yticklabels=['No Dropout', 'Dropout'])\n",
        "        axes[0, 1].set_title('Confusion Matrix')\n",
        "        axes[0, 1].set_ylabel('True Label')\n",
        "        axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "        # Plot 3: ROC Curve\n",
        "        fpr, tpr, _ = roc_curve(self.results['y_test'], self.results['y_pred_proba'])\n",
        "        axes[0, 2].plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                       label=f'ROC curve (AUC = {self.results[\"roc_auc\"]:.3f})')\n",
        "        axes[0, 2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        axes[0, 2].set_xlim([0.0, 1.0])\n",
        "        axes[0, 2].set_ylim([0.0, 1.05])\n",
        "        axes[0, 2].set_xlabel('False Positive Rate')\n",
        "        axes[0, 2].set_ylabel('True Positive Rate')\n",
        "        axes[0, 2].set_title('ROC Curve')\n",
        "        axes[0, 2].legend(loc=\"lower right\")\n",
        "\n",
        "        # Plot 4: Training History (if available)\n",
        "        if hasattr(self.best_model, 'history') and self.best_model.history:\n",
        "            history = self.best_model.history.history\n",
        "\n",
        "            axes[1, 0].plot(history['loss'], label='Training Loss')\n",
        "            axes[1, 0].plot(history['val_loss'], label='Validation Loss')\n",
        "            axes[1, 0].set_title('Training History - Loss')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Loss')\n",
        "            axes[1, 0].legend()\n",
        "\n",
        "            axes[1, 1].plot(history['accuracy'], label='Training Accuracy')\n",
        "            axes[1, 1].plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "            axes[1, 1].set_title('Training History - Accuracy')\n",
        "            axes[1, 1].set_xlabel('Epoch')\n",
        "            axes[1, 1].set_ylabel('Accuracy')\n",
        "            axes[1, 1].legend()\n",
        "\n",
        "        # Plot 5: Prediction Distribution\n",
        "        axes[1, 2].hist(self.results['y_pred_proba'], bins=30, alpha=0.7, color='lightblue')\n",
        "        axes[1, 2].set_title('Prediction Probability Distribution')\n",
        "        axes[1, 2].set_xlabel('Predicted Dropout Probability')\n",
        "        axes[1, 2].set_ylabel('Frequency')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def run_complete_pipeline(self, csv_file_path, n_iter=30, cv_folds=3):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ATTENTION-LSTM WITH RANDOMIZEDSEARCHCV PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load and preprocess data\n",
        "        X, y = self.load_and_preprocess_data(csv_file_path)\n",
        "\n",
        "        # Run RandomizedSearchCV\n",
        "        self.run_randomized_search(X, y, n_iter=n_iter, cv_folds=cv_folds)\n",
        "\n",
        "        # Train best model\n",
        "        results = self.train_best_model(X, y)\n",
        "\n",
        "        # Plot results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"GENERATING VISUALIZATIONS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self.plot_hyperparameter_analysis()\n",
        "        self.plot_model_performance()\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nDetailed Classification Report:\")\n",
        "        print(classification_report(\n",
        "            self.results['y_test'],\n",
        "            self.results['y_pred'],\n",
        "            target_names=['No Dropout', 'Dropout']\n",
        "        ))\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the predictor\n",
        "    predictor = StudentDropoutPredictor(\n",
        "        sequence_length=30,\n",
        "        features_per_day=7\n",
        "    )\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    csv_file_path = 'model1_210_features.csv'\n",
        "\n",
        "    try:\n",
        "        results = predictor.run_complete_pipeline(\n",
        "            csv_file_path=csv_file_path,\n",
        "            n_iter=20,  # Reduced for faster execution, increase for better results\n",
        "            cv_folds=3\n",
        "        )\n",
        "\n",
        "        print(\"\\nBest Model Configuration:\")\n",
        "        for param, value in results['best_params'].items():\n",
        "            print(f\"{param}: {value}\")\n",
        "\n",
        "        print(f\"\\nFinal Performance:\")\n",
        "        print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "        print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
        "        print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find the file '{csv_file_path}'\")\n",
        "        print(\"Please make sure the CSV file is in the correct location.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    # Example of making predictions on new data\n",
        "    \"\"\"\n",
        "    if predictor.best_model:\n",
        "        # Create dummy test data (replace with real data)\n",
        "        test_sequences = np.random.rand(5, 30, 7)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = predictor.best_model.predict(test_sequences)\n",
        "        probabilities = predictor.best_model.predict_proba(test_sequences)[:, 1]\n",
        "\n",
        "        print(\"\\nSample Predictions:\")\n",
        "        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
        "            risk_level = \"High\" if prob > 0.7 else \"Medium\" if prob > 0.3 else \"Low\"\n",
        "            print(f\"Student {i+1}: Prediction={pred}, Probability={prob:.3f} ({risk_level} risk)\")\n",
        "    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSmtNQQvPz2I",
        "outputId": "d4b3b0e6-8074-4dee-d639-39a76a43e607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ATTENTION-LSTM WITH RANDOMIZEDSEARCHCV PIPELINE\n",
            "============================================================\n",
            "Loading and preprocessing data...\n",
            "Loaded 120542 samples with 214 columns\n",
            "Class distribution: {np.int64(0): np.int64(24961), np.int64(1): np.int64(95581)}\n",
            "Imbalance ratio: 3.83\n",
            "Starting RandomizedSearchCV with 20 iterations...\n",
            "Fitting RandomizedSearchCV...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        }
      ]
    }
  ]
}