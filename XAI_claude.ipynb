{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/doctorat/blob/main/XAI_claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxOkIC5VihGs",
        "outputId": "e21cc2a1-986f-49d3-c6b8-1224d580a7b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn xgboost lightgbm catboost\n",
        "!pip install imbalanced-learn shap matplotlib seaborn scipy optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "P-sMZb-dhetQ",
        "outputId": "a6f10af5-7308-4423-a80a-579139d3d10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting Student Dropout Prediction Pipeline\n",
            "============================================================\n",
            "ğŸ¯ Student Dropout Prediction - Best Accuracy Pipeline\n",
            "Optimized for maximum performance with advanced ML techniques\n",
            "\n",
            "ğŸ“Š Loading and preprocessing data...\n",
            "Original dataset shape: (120542, 214)\n",
            "After dropping metadata columns: (120542, 211)\n",
            "Missing values: 0\n",
            "\n",
            "Dataset Statistics:\n",
            "- Total students: 120,542\n",
            "- Total features: 210\n",
            "- Dropout rate: 79.3%\n",
            "- Class distribution: {1: np.int64(95581), 0: np.int64(24961)}\n",
            "- Imbalance ratio: 1:0.26\n",
            "\n",
            "ğŸ”§ Creating advanced features...\n",
            "   - Creating weekly aggregations...\n",
            "   - Creating activity type aggregations...\n",
            "   - Creating temporal patterns...\n",
            "   - Creating interaction features...\n",
            "   - Creating rolling statistics...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Rolling.apply() got an unexpected keyword argument 'axis'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1566441451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;31m# Run the complete pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m     \u001b[0mpipeline_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;31m# Optional: Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1566441451.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# 2. Advanced feature engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m     \u001b[0mdf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_advanced_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;31m# 3. Prepare data for modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1566441451.py\u001b[0m in \u001b[0;36mcreate_advanced_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Rolling trends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         df_features['rolling_3_trend'] = daily_df.rolling(3, axis=1).apply(\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         ).mean(axis=1)\n",
            "\u001b[0;31mTypeError\u001b[0m: Rolling.apply() got an unexpected keyword argument 'axis'"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Student Dropout Prediction Pipeline\n",
        "Optimized for Best Accuracy Performance\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, StratifiedKFold, cross_val_score,\n",
        "    RandomizedSearchCV, GridSearchCV\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, balanced_accuracy_score\n",
        ")\n",
        "\n",
        "# Imbalanced data handling\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Advanced models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Hyperparameter optimization\n",
        "from scipy.stats import uniform, randint, loguniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Explainable AI\n",
        "import shap\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"ğŸš€ Starting Student Dropout Prediction Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load data and perform initial preprocessing\"\"\"\n",
        "    print(\"\\nğŸ“Š Loading and preprocessing data...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "    # Drop specified columns\n",
        "    columns_to_drop = ['username', 'course_id', 'enrollment_id']\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    print(f\"After dropping metadata columns: {df.shape}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum().sum()\n",
        "    print(f\"Missing values: {missing_values}\")\n",
        "\n",
        "    if missing_values > 0:\n",
        "        # Forward fill for time series nature of data\n",
        "        df = df.fillna(method='ffill').fillna(0)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"- Total students: {len(df):,}\")\n",
        "    print(f\"- Total features: {df.shape[1] - 1}\")  # Excluding target\n",
        "\n",
        "    # Target distribution\n",
        "    dropout_counts = df['dropout'].value_counts()\n",
        "    dropout_rate = dropout_counts[1] / len(df)\n",
        "    print(f\"- Dropout rate: {dropout_rate:.1%}\")\n",
        "    print(f\"- Class distribution: {dict(dropout_counts)}\")\n",
        "    print(f\"- Imbalance ratio: 1:{dropout_counts[0]/dropout_counts[1]:.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ADVANCED FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "def create_advanced_features(df):\n",
        "    \"\"\"Create comprehensive feature engineering\"\"\"\n",
        "    print(\"\\nğŸ”§ Creating advanced features...\")\n",
        "\n",
        "    df_features = df.copy()\n",
        "\n",
        "    # Get activity columns (all except dropout)\n",
        "    activity_cols = [col for col in df.columns if col != 'dropout']\n",
        "    activity_types = ['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']\n",
        "\n",
        "    # 1. WEEKLY AGGREGATIONS\n",
        "    print(\"   - Creating weekly aggregations...\")\n",
        "    for week in range(1, 5):  # 4 weeks\n",
        "        week_start = (week - 1) * 7 + 1\n",
        "        week_end = min(week * 7, 30)\n",
        "        week_cols = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(week_start, week_end + 1))]\n",
        "\n",
        "        if week_cols:\n",
        "            df_features[f'week_{week}_total'] = df_features[week_cols].sum(axis=1)\n",
        "            df_features[f'week_{week}_mean'] = df_features[week_cols].mean(axis=1)\n",
        "            df_features[f'week_{week}_std'] = df_features[week_cols].std(axis=1).fillna(0)\n",
        "            df_features[f'week_{week}_max'] = df_features[week_cols].max(axis=1)\n",
        "            df_features[f'week_{week}_min'] = df_features[week_cols].min(axis=1)\n",
        "            df_features[f'week_{week}_range'] = df_features[f'week_{week}_max'] - df_features[f'week_{week}_min']\n",
        "\n",
        "    # 2. ACTIVITY TYPE AGGREGATIONS\n",
        "    print(\"   - Creating activity type aggregations...\")\n",
        "    for activity in activity_types:\n",
        "        activity_cols_type = [col for col in activity_cols if activity in col]\n",
        "        if activity_cols_type:\n",
        "            df_features[f'total_{activity}'] = df_features[activity_cols_type].sum(axis=1)\n",
        "            df_features[f'mean_{activity}'] = df_features[activity_cols_type].mean(axis=1)\n",
        "            df_features[f'std_{activity}'] = df_features[activity_cols_type].std(axis=1).fillna(0)\n",
        "            df_features[f'max_{activity}'] = df_features[activity_cols_type].max(axis=1)\n",
        "\n",
        "            # Early activity (days 1-3)\n",
        "            early_cols = [col for col in activity_cols_type if any(f'day_{d}_' in col for d in range(1, 4))]\n",
        "            if early_cols:\n",
        "                df_features[f'early_{activity}'] = df_features[early_cols].sum(axis=1)\n",
        "\n",
        "            # Late activity (days 21-30)\n",
        "            late_cols = [col for col in activity_cols_type if any(f'day_{d}_' in col for d in range(21, 31))]\n",
        "            if late_cols:\n",
        "                df_features[f'late_{activity}'] = df_features[late_cols].sum(axis=1)\n",
        "\n",
        "    # 3. TEMPORAL PATTERNS\n",
        "    print(\"   - Creating temporal patterns...\")\n",
        "\n",
        "    # Early engagement (critical period)\n",
        "    early_days = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(1, 4))]\n",
        "    df_features['early_engagement'] = df_features[early_days].sum(axis=1)\n",
        "\n",
        "    # First week vs last week comparison\n",
        "    first_week = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(1, 8))]\n",
        "    last_week = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(24, 31))]\n",
        "\n",
        "    df_features['first_week_total'] = df_features[first_week].sum(axis=1)\n",
        "    df_features['last_week_total'] = df_features[last_week].sum(axis=1)\n",
        "    df_features['engagement_decline'] = (df_features['first_week_total'] - df_features['last_week_total']) / (df_features['first_week_total'] + 1)\n",
        "\n",
        "    # Activity consistency (coefficient of variation)\n",
        "    for activity in activity_types:\n",
        "        activity_cols_type = [col for col in activity_cols if activity in col]\n",
        "        if activity_cols_type:\n",
        "            means = df_features[activity_cols_type].mean(axis=1)\n",
        "            stds = df_features[activity_cols_type].std(axis=1).fillna(0)\n",
        "            df_features[f'cv_{activity}'] = stds / (means + 1)  # Coefficient of variation\n",
        "\n",
        "    # 4. INTERACTION FEATURES\n",
        "    print(\"   - Creating interaction features...\")\n",
        "\n",
        "    # Problem-solving to video watching ratio\n",
        "    if 'total_problem' in df_features.columns and 'total_video' in df_features.columns:\n",
        "        df_features['problem_video_ratio'] = df_features['total_problem'] / (df_features['total_video'] + 1)\n",
        "\n",
        "    # Discussion to access ratio (social engagement)\n",
        "    if 'total_discussion' in df_features.columns and 'total_access' in df_features.columns:\n",
        "        df_features['discussion_access_ratio'] = df_features['total_discussion'] / (df_features['total_access'] + 1)\n",
        "\n",
        "    # Wiki usage efficiency\n",
        "    if 'total_wiki' in df_features.columns and 'total_access' in df_features.columns:\n",
        "        df_features['wiki_efficiency'] = df_features['total_wiki'] / (df_features['total_access'] + 1)\n",
        "\n",
        "    # 5. ROLLING STATISTICS\n",
        "    print(\"   - Creating rolling statistics...\")\n",
        "\n",
        "    # Create daily totals for rolling calculations\n",
        "    daily_totals = []\n",
        "    for day in range(1, 31):\n",
        "        day_cols = [col for col in activity_cols if f'day_{day}_' in col]\n",
        "        if day_cols:\n",
        "            daily_totals.append(df_features[day_cols].sum(axis=1))\n",
        "\n",
        "    if daily_totals:\n",
        "        daily_df = pd.concat(daily_totals, axis=1)\n",
        "        daily_df.columns = [f'day_{i+1}_total' for i in range(len(daily_totals))]\n",
        "\n",
        "        # Rolling averages\n",
        "        df_features['rolling_3_mean'] = daily_df.rolling(3, axis=1).mean().mean(axis=1)\n",
        "        df_features['rolling_7_mean'] = daily_df.rolling(7, axis=1).mean().mean(axis=1)\n",
        "\n",
        "        # Rolling trends\n",
        "        df_features['rolling_3_trend'] = daily_df.rolling(3, axis=1).apply(\n",
        "            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 3 else 0, axis=1\n",
        "        ).mean(axis=1)\n",
        "\n",
        "    # 6. STATISTICAL FEATURES\n",
        "    print(\"   - Creating statistical features...\")\n",
        "\n",
        "    # Overall activity statistics\n",
        "    all_activity_cols = [col for col in activity_cols if 'day_' in col]\n",
        "    df_features['total_activity'] = df_features[all_activity_cols].sum(axis=1)\n",
        "    df_features['mean_daily_activity'] = df_features[all_activity_cols].mean(axis=1)\n",
        "    df_features['std_daily_activity'] = df_features[all_activity_cols].std(axis=1).fillna(0)\n",
        "    df_features['skew_activity'] = df_features[all_activity_cols].skew(axis=1).fillna(0)\n",
        "    df_features['kurtosis_activity'] = df_features[all_activity_cols].kurtosis(axis=1).fillna(0)\n",
        "\n",
        "    # Activity streaks (consecutive days with activity)\n",
        "    df_features['max_activity_streak'] = (df_features[all_activity_cols] > 0).astype(int).apply(\n",
        "        lambda row: max([len(list(g)) for k, g in __import__('itertools').groupby(row) if k] + [0]), axis=1\n",
        "    )\n",
        "\n",
        "    print(f\"   - Total features after engineering: {df_features.shape[1] - 1}\")\n",
        "    print(f\"   - Added {df_features.shape[1] - df.shape[1]} new features\")\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ADVANCED MODEL TRAINING WITH OPTIMIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_models_with_hyperparameter_optimization(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train multiple models with advanced hyperparameter optimization\"\"\"\n",
        "    print(\"\\nğŸ¤– Training models with hyperparameter optimization...\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Define sampling strategies\n",
        "    sampling_strategies = {\n",
        "        'SMOTE': SMOTE(random_state=RANDOM_STATE, k_neighbors=3),\n",
        "        'ADASYN': ADASYN(random_state=RANDOM_STATE),\n",
        "        'BorderlineSMOTE': BorderlineSMOTE(random_state=RANDOM_STATE),\n",
        "        'SMOTEENN': SMOTEENN(random_state=RANDOM_STATE),\n",
        "        'SMOTETomek': SMOTETomek(random_state=RANDOM_STATE)\n",
        "    }\n",
        "\n",
        "    # XGBoost with extensive hyperparameter tuning\n",
        "    print(\"\\n   ğŸ” Optimizing XGBoost...\")\n",
        "\n",
        "    xgb_param_dist = {\n",
        "        'classifier__n_estimators': randint(200, 2000),\n",
        "        'classifier__max_depth': randint(3, 15),\n",
        "        'classifier__learning_rate': loguniform(0.01, 0.3),\n",
        "        'classifier__subsample': uniform(0.6, 0.35),\n",
        "        'classifier__colsample_bytree': uniform(0.6, 0.35),\n",
        "        'classifier__colsample_bylevel': uniform(0.6, 0.35),\n",
        "        'classifier__min_child_weight': randint(1, 10),\n",
        "        'classifier__gamma': uniform(0, 1),\n",
        "        'classifier__reg_alpha': loguniform(1e-8, 10),\n",
        "        'classifier__reg_lambda': loguniform(1e-8, 10)\n",
        "    }\n",
        "\n",
        "    best_xgb_score = 0\n",
        "    best_xgb_model = None\n",
        "\n",
        "    for sampling_name, sampling in sampling_strategies.items():\n",
        "        print(f\"      Testing XGBoost + {sampling_name}...\")\n",
        "\n",
        "        pipeline = ImbPipeline([\n",
        "            ('sampling', sampling),\n",
        "            ('classifier', xgb.XGBClassifier(\n",
        "                random_state=RANDOM_STATE,\n",
        "                eval_metric='logloss',\n",
        "                tree_method='hist',\n",
        "                objective='binary:logistic'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            xgb_param_dist,\n",
        "            n_iter=100,  # Increased iterations\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if search.best_score_ > best_xgb_score:\n",
        "            best_xgb_score = search.best_score_\n",
        "            best_xgb_model = search.best_estimator_\n",
        "            best_xgb_sampling = sampling_name\n",
        "\n",
        "    # Evaluate best XGBoost\n",
        "    y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "    y_pred_proba_xgb = best_xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results['XGBoost'] = {\n",
        "        'model': best_xgb_model,\n",
        "        'sampling': best_xgb_sampling,\n",
        "        'cv_score': best_xgb_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_xgb),\n",
        "        'test_precision': precision_score(y_test, y_pred_xgb),\n",
        "        'test_recall': recall_score(y_test, y_pred_xgb),\n",
        "        'test_f1': f1_score(y_test, y_pred_xgb),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_xgb),\n",
        "        'predictions': y_pred_xgb,\n",
        "        'probabilities': y_pred_proba_xgb\n",
        "    }\n",
        "\n",
        "    print(f\"      Best XGBoost: {best_xgb_sampling} - CV AUC: {best_xgb_score:.4f}, Test AUC: {results['XGBoost']['test_auc']:.4f}\")\n",
        "\n",
        "    # LightGBM optimization\n",
        "    print(\"\\n   ğŸ” Optimizing LightGBM...\")\n",
        "\n",
        "    lgb_param_dist = {\n",
        "        'classifier__n_estimators': randint(200, 2000),\n",
        "        'classifier__max_depth': randint(3, 15),\n",
        "        'classifier__learning_rate': loguniform(0.01, 0.3),\n",
        "        'classifier__num_leaves': randint(20, 300),\n",
        "        'classifier__min_child_samples': randint(10, 200),\n",
        "        'classifier__min_child_weight': loguniform(1e-5, 10),\n",
        "        'classifier__subsample': uniform(0.6, 0.35),\n",
        "        'classifier__colsample_bytree': uniform(0.6, 0.35),\n",
        "        'classifier__reg_alpha': loguniform(1e-8, 10),\n",
        "        'classifier__reg_lambda': loguniform(1e-8, 10)\n",
        "    }\n",
        "\n",
        "    best_lgb_score = 0\n",
        "    best_lgb_model = None\n",
        "\n",
        "    for sampling_name, sampling in sampling_strategies.items():\n",
        "        pipeline = ImbPipeline([\n",
        "            ('sampling', sampling),\n",
        "            ('classifier', lgb.LGBMClassifier(\n",
        "                random_state=RANDOM_STATE,\n",
        "                verbose=-1,\n",
        "                objective='binary',\n",
        "                boosting_type='gbdt'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            lgb_param_dist,\n",
        "            n_iter=80,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if search.best_score_ > best_lgb_score:\n",
        "            best_lgb_score = search.best_score_\n",
        "            best_lgb_model = search.best_estimator_\n",
        "            best_lgb_sampling = sampling_name\n",
        "\n",
        "    # Evaluate best LightGBM\n",
        "    y_pred_lgb = best_lgb_model.predict(X_test)\n",
        "    y_pred_proba_lgb = best_lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results['LightGBM'] = {\n",
        "        'model': best_lgb_model,\n",
        "        'sampling': best_lgb_sampling,\n",
        "        'cv_score': best_lgb_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_lgb),\n",
        "        'test_precision': precision_score(y_test, y_pred_lgb),\n",
        "        'test_recall': recall_score(y_test, y_pred_lgb),\n",
        "        'test_f1': f1_score(y_test, y_pred_lgb),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_lgb),\n",
        "        'predictions': y_pred_lgb,\n",
        "        'probabilities': y_pred_proba_lgb\n",
        "    }\n",
        "\n",
        "    print(f\"      Best LightGBM: {best_lgb_sampling} - CV AUC: {best_lgb_score:.4f}, Test AUC: {results['LightGBM']['test_auc']:.4f}\")\n",
        "\n",
        "    # CatBoost optimization\n",
        "    print(\"\\n   ğŸ” Optimizing CatBoost...\")\n",
        "\n",
        "    catboost_param_dist = {\n",
        "        'classifier__iterations': randint(200, 1500),\n",
        "        'classifier__depth': randint(4, 12),\n",
        "        'classifier__learning_rate': loguniform(0.01, 0.3),\n",
        "        'classifier__l2_leaf_reg': loguniform(1, 30),\n",
        "        'classifier__border_count': randint(32, 255),\n",
        "        'classifier__bagging_temperature': uniform(0, 10)\n",
        "    }\n",
        "\n",
        "    best_cat_score = 0\n",
        "    best_cat_model = None\n",
        "\n",
        "    for sampling_name, sampling in list(sampling_strategies.items())[:3]:  # Test top 3 sampling methods\n",
        "        pipeline = ImbPipeline([\n",
        "            ('sampling', sampling),\n",
        "            ('classifier', CatBoostClassifier(\n",
        "                random_state=RANDOM_STATE,\n",
        "                verbose=False,\n",
        "                objective='Logloss',\n",
        "                eval_metric='AUC'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            catboost_param_dist,\n",
        "            n_iter=60,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if search.best_score_ > best_cat_score:\n",
        "            best_cat_score = search.best_score_\n",
        "            best_cat_model = search.best_estimator_\n",
        "            best_cat_sampling = sampling_name\n",
        "\n",
        "    # Evaluate best CatBoost\n",
        "    y_pred_cat = best_cat_model.predict(X_test)\n",
        "    y_pred_proba_cat = best_cat_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results['CatBoost'] = {\n",
        "        'model': best_cat_model,\n",
        "        'sampling': best_cat_sampling,\n",
        "        'cv_score': best_cat_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_cat),\n",
        "        'test_precision': precision_score(y_test, y_pred_cat),\n",
        "        'test_recall': recall_score(y_test, y_pred_cat),\n",
        "        'test_f1': f1_score(y_test, y_pred_cat),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_cat),\n",
        "        'predictions': y_pred_cat,\n",
        "        'probabilities': y_pred_proba_cat\n",
        "    }\n",
        "\n",
        "    print(f\"      Best CatBoost: {best_cat_sampling} - CV AUC: {best_cat_score:.4f}, Test AUC: {results['CatBoost']['test_auc']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ENSEMBLE MODEL CREATION\n",
        "# ============================================================================\n",
        "\n",
        "def create_ensemble_model(models_results, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Create ensemble model from best individual models\"\"\"\n",
        "    print(\"\\nğŸ¯ Creating ensemble model...\")\n",
        "\n",
        "    # Get top 3 models by AUC\n",
        "    sorted_models = sorted(models_results.items(), key=lambda x: x[1]['test_auc'], reverse=True)\n",
        "    top_models = sorted_models[:3]\n",
        "\n",
        "    print(\"   Top models for ensemble:\")\n",
        "    for name, result in top_models:\n",
        "        print(f\"      - {name}: AUC {result['test_auc']:.4f}\")\n",
        "\n",
        "    # Create voting ensemble\n",
        "    estimators = [(name, result['model']) for name, result in top_models]\n",
        "\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=estimators,\n",
        "        voting='soft'  # Use probability voting\n",
        "    )\n",
        "\n",
        "    # Train ensemble\n",
        "    ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    y_pred_ensemble = ensemble.predict(X_test)\n",
        "    y_pred_proba_ensemble = ensemble.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    ensemble_results = {\n",
        "        'model': ensemble,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_ensemble),\n",
        "        'test_precision': precision_score(y_test, y_pred_ensemble),\n",
        "        'test_recall': recall_score(y_test, y_pred_ensemble),\n",
        "        'test_f1': f1_score(y_test, y_pred_ensemble),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_ensemble),\n",
        "        'predictions': y_pred_ensemble,\n",
        "        'probabilities': y_pred_proba_ensemble\n",
        "    }\n",
        "\n",
        "    print(f\"   Ensemble AUC: {ensemble_results['test_auc']:.4f}\")\n",
        "\n",
        "    return ensemble_results\n",
        "\n",
        "# ============================================================================\n",
        "# 5. EXPLAINABLE AI ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "def explain_best_model(best_model, X_train, X_test, feature_names):\n",
        "    \"\"\"Generate SHAP explanations for the best model\"\"\"\n",
        "    print(\"\\nğŸ” Generating SHAP explanations...\")\n",
        "\n",
        "    try:\n",
        "        # Get the classifier from the pipeline\n",
        "        if hasattr(best_model, 'named_steps'):\n",
        "            classifier = best_model.named_steps['classifier']\n",
        "            # Apply sampling to get training data for explainer\n",
        "            sampler = best_model.named_steps['sampling']\n",
        "            X_resampled, _ = sampler.fit_resample(X_train, y_train)\n",
        "        else:\n",
        "            classifier = best_model\n",
        "            X_resampled = X_train\n",
        "\n",
        "        # Create explainer based on model type\n",
        "        if isinstance(classifier, (xgb.XGBClassifier, lgb.LGBMClassifier)):\n",
        "            explainer = shap.TreeExplainer(classifier)\n",
        "        else:\n",
        "            # For CatBoost or other models, use a sample for background\n",
        "            explainer = shap.Explainer(classifier, X_resampled[:100])\n",
        "\n",
        "        # Calculate SHAP values for a sample of test data\n",
        "        sample_size = min(500, len(X_test))\n",
        "        X_sample = X_test[:sample_size]\n",
        "\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "        # Handle different SHAP output formats\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_values = shap_values[1]  # Binary classification positive class\n",
        "\n",
        "        # Calculate feature importance\n",
        "        feature_importance = np.abs(shap_values).mean(axis=0)\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': feature_importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"   Top 15 most important features:\")\n",
        "        print(importance_df.head(15).to_string(index=False))\n",
        "\n",
        "        return importance_df, shap_values\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error in SHAP analysis: {e}\")\n",
        "        # Fallback to permutation importance\n",
        "        from sklearn.inspection import permutation_importance\n",
        "\n",
        "        perm_importance = permutation_importance(\n",
        "            best_model, X_test, y_test,\n",
        "            n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1\n",
        "        )\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': perm_importance.importances_mean\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"   Top 15 most important features (permutation importance):\")\n",
        "        print(importance_df.head(15).to_string(index=False))\n",
        "\n",
        "        return importance_df, None\n",
        "\n",
        "# ============================================================================\n",
        "# 6. RESULTS ANALYSIS AND REPORTING\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_and_report_results(models_results, ensemble_results, feature_importance):\n",
        "    \"\"\"Generate comprehensive results analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“Š FINAL RESULTS ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = models_results.copy()\n",
        "    all_results['Ensemble'] = ensemble_results\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Model': list(all_results.keys()),\n",
        "        'Accuracy': [result['test_accuracy'] for result in all_results.values()],\n",
        "        'Precision': [result['test_precision'] for result in all_results.values()],\n",
        "        'Recall': [result['test_recall'] for result in all_results.values()],\n",
        "        'F1-Score': [result['test_f1'] for result in all_results.values()],\n",
        "        'AUC-ROC': [result['test_auc'] for result in all_results.values()]\n",
        "    })\n",
        "\n",
        "    results_df = results_df.round(4)\n",
        "    results_df = results_df.sort_values('AUC-ROC', ascending=False)\n",
        "\n",
        "    print(\"\\nğŸ† MODEL PERFORMANCE COMPARISON:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Best model\n",
        "    best_model_name = results_df.iloc[0]['Model']\n",
        "    best_auc = results_df.iloc[0]['AUC-ROC']\n",
        "    best_accuracy = results_df.iloc[0]['Accuracy']\n",
        "\n",
        "    print(f\"\\nğŸ¥‡ BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   - AUC-ROC: {best_auc:.4f}\")\n",
        "    print(f\"   - Accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"   - Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
        "    print(f\"   - Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
        "    print(f\"   - F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "\n",
        "    # Feature importance insights\n",
        "    print(f\"\\nğŸ” KEY INSIGHTS FROM FEATURE IMPORTANCE:\")\n",
        "\n",
        "    # Categorize features\n",
        "    early_features = feature_importance[feature_importance['feature'].str.contains('day_[1-3]|early_')].head(5)\n",
        "    activity_features = feature_importance[feature_importance['feature'].str.contains('total_|mean_')].head(5)\n",
        "    temporal_features = feature_importance[feature_importance['feature'].str.contains('week_|decline|trend')].head(5)\n",
        "\n",
        "    if not early_features.empty:\n",
        "        print(f\"   ğŸš¨ Early Warning Indicators:\")\n",
        "        for _, row in early_features.iterrows():\n",
        "            print(f\"      - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "    if not activity_features.empty:\n",
        "        print(f\"   ğŸ“Š Activity Pattern Indicators:\")\n",
        "        for _, row in activity_features.iterrows():\n",
        "            print(f\"      - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "    if not temporal_features.empty:\n",
        "        print(f\"   ğŸ“ˆ Temporal Pattern Indicators:\")\n",
        "        for _, row in temporal_features.iterrows():\n",
        "            print(f\"      - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "    # Performance improvements\n",
        "    baseline_auc = min(results_df['AUC-ROC'])\n",
        "    improvement = ((best_auc - baseline_auc) / baseline_auc) * 100\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ PERFORMANCE IMPROVEMENTS:\")\n",
        "    print(f\"   - Best model improvement over baseline: {improvement:.2f}%\")\n",
        "    print(f\"   - Achieved target of >90% AUC: {'âœ… YES' if best_auc > 0.90 else 'âŒ NO'}\")\n",
        "    print(f\"   - Model ready for production: {'âœ… YES' if best_auc > 0.85 else 'âŒ NO'}\")\n",
        "\n",
        "    return results_df, best_model_name\n",
        "\n",
        "# ============================================================================\n",
        "# 7. MAIN EXECUTION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"ğŸ¯ Student Dropout Prediction - Best Accuracy Pipeline\")\n",
        "    print(\"Optimized for maximum performance with advanced ML techniques\")\n",
        "\n",
        "    # 1. Load and preprocess data\n",
        "    df = load_and_preprocess_data('model1_210_features.csv')\n",
        "\n",
        "    # 2. Advanced feature engineering\n",
        "    df_features = create_advanced_features(df)\n",
        "\n",
        "    # 3. Prepare data for modeling\n",
        "    print(\"\\nâš™ï¸ Preparing data for modeling...\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df_features.drop('dropout', axis=1)\n",
        "    y = df_features['dropout']\n",
        "\n",
        "    print(f\"Final feature count: {X.shape[1]}\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE,\n",
        "        stratify=y, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = RobustScaler()  # More robust to outliers\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "    # 4. Train models with hyperparameter optimization\n",
        "    models_results = train_models_with_hyperparameter_optimization(\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # 5. Create ensemble model\n",
        "    ensemble_results = create_ensemble_model(\n",
        "        models_results, X_train_scaled, X_test_scaled, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # 6. Determine best model\n",
        "    all_results = models_results.copy()\n",
        "    all_results['Ensemble'] = ensemble_results\n",
        "\n",
        "    best_model_name = max(all_results.keys(), key=lambda k: all_results[k]['test_auc'])\n",
        "    best_model = all_results[best_model_name]['model']\n",
        "\n",
        "    print(f\"\\nğŸ† Best performing model: {best_model_name}\")\n",
        "    print(f\"Best AUC: {all_results[best_model_name]['test_auc']:.4f}\")\n",
        "\n",
        "    # 7. Explainable AI analysis\n",
        "    feature_importance, shap_values = explain_best_model(\n",
        "        best_model, X_train_scaled, X_test_scaled, X.columns.tolist()\n",
        "    )\n",
        "\n",
        "    # 8. Final analysis and reporting\n",
        "    results_df, final_best_model = analyze_and_report_results(\n",
        "        models_results, ensemble_results, feature_importance\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'best_model': best_model,\n",
        "        'best_model_name': final_best_model,\n",
        "        'feature_importance': feature_importance,\n",
        "        'scaler': scaler,\n",
        "        'models_results': all_results\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# 8. EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    pipeline_results = main()\n",
        "\n",
        "    # Optional: Save results\n",
        "    try:\n",
        "        pipeline_results['results_df'].to_csv('dropout_prediction_results.csv', index=False)\n",
        "        pipeline_results['feature_importance'].to_csv('feature_importance.csv', index=False)\n",
        "        print(\"\\nğŸ’¾ Results saved to CSV files\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸ Could not save results: {e}\")\n",
        "\n",
        "    print(f\"\\nğŸš€ Best model achieved AUC: {pipeline_results['results_df'].iloc[0]['AUC-ROC']:.4f}\")\n",
        "    print(\"Pipeline ready for production deployment! ğŸ¯\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMvQ-Wzkllqy",
        "outputId": "48a2246b-1eed-4fd7-d39a-228195d6c691"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting Student Dropout Prediction Pipeline\n",
            "============================================================\n",
            "ğŸ¯ Student Dropout Prediction - Best Accuracy Pipeline\n",
            "Optimized for maximum performance with advanced ML techniques\n",
            "\n",
            "ğŸ“Š Loading and preprocessing data...\n",
            "Original dataset shape: (120542, 214)\n",
            "After dropping metadata columns: (120542, 211)\n",
            "Missing values: 0\n",
            "\n",
            "Dataset Statistics:\n",
            "- Total students: 120,542\n",
            "- Total features: 210\n",
            "- Dropout rate: 79.3%\n",
            "- Class distribution: {1: np.int64(95581), 0: np.int64(24961)}\n",
            "- Imbalance ratio: 1:0.26\n",
            "\n",
            "ğŸ”§ Creating advanced features...\n",
            "   - Creating weekly aggregations...\n",
            "   - Creating activity type aggregations...\n",
            "   - Creating temporal patterns...\n",
            "   - Creating interaction features...\n",
            "   - Creating rolling statistics...\n",
            "   - Creating statistical features...\n",
            "   - Total features after engineering: 299\n",
            "   - Added 89 new features\n",
            "\n",
            "âš™ï¸ Preparing data for modeling...\n",
            "Final feature count: 299\n",
            "Training set: 96,433 samples\n",
            "Test set: 24,109 samples\n",
            "\n",
            "ğŸ¤– Training models with hyperparameter optimization...\n",
            "\n",
            "   ğŸ” Optimizing XGBoost...\n",
            "      Testing XGBoost + SMOTE...\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Student Dropout Prediction Pipeline\n",
        "Optimized for Best Accuracy Performance\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, StratifiedKFold, cross_val_score,\n",
        "    RandomizedSearchCV, GridSearchCV\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, balanced_accuracy_score\n",
        ")\n",
        "\n",
        "# Imbalanced data handling\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Advanced models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Hyperparameter optimization\n",
        "from scipy.stats import uniform, randint, loguniform\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Explainable AI\n",
        "import shap\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"ğŸš€ Starting Student Dropout Prediction Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load data and perform initial preprocessing\"\"\"\n",
        "    print(\"\\nğŸ“Š Loading and preprocessing data...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "    # Drop specified columns\n",
        "    columns_to_drop = ['username', 'course_id', 'enrollment_id']\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "    print(f\"After dropping metadata columns: {df.shape}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum().sum()\n",
        "    print(f\"Missing values: {missing_values}\")\n",
        "\n",
        "    if missing_values > 0:\n",
        "        # Handle missing values - use different methods for compatibility\n",
        "        try:\n",
        "            # Try newer pandas syntax first\n",
        "            df = df.fillna(method='ffill').fillna(0)\n",
        "        except TypeError:\n",
        "            # Fallback for newer pandas versions\n",
        "            df = df.ffill().fillna(0)\n",
        "        except:\n",
        "            # Final fallback - simple forward fill\n",
        "            df = df.fillna(df.mean()).fillna(0)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"- Total students: {len(df):,}\")\n",
        "    print(f\"- Total features: {df.shape[1] - 1}\")  # Excluding target\n",
        "\n",
        "    # Target distribution\n",
        "    dropout_counts = df['dropout'].value_counts()\n",
        "    dropout_rate = dropout_counts[1] / len(df)\n",
        "    print(f\"- Dropout rate: {dropout_rate:.1%}\")\n",
        "    print(f\"- Class distribution: {dict(dropout_counts)}\")\n",
        "    print(f\"- Imbalance ratio: 1:{dropout_counts[0]/dropout_counts[1]:.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ADVANCED FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "def create_advanced_features(df):\n",
        "    \"\"\"Create comprehensive feature engineering\"\"\"\n",
        "    print(\"\\nğŸ”§ Creating advanced features...\")\n",
        "\n",
        "    df_features = df.copy()\n",
        "\n",
        "    # Get activity columns (all except dropout)\n",
        "    activity_cols = [col for col in df.columns if col != 'dropout']\n",
        "    activity_types = ['access', 'problem', 'wiki', 'discussion', 'navigate', 'page_close', 'video']\n",
        "\n",
        "    # 1. WEEKLY AGGREGATIONS\n",
        "    print(\"   - Creating weekly aggregations...\")\n",
        "    for week in range(1, 5):  # 4 weeks\n",
        "        week_start = (week - 1) * 7 + 1\n",
        "        week_end = min(week * 7, 30)\n",
        "        week_cols = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(week_start, week_end + 1))]\n",
        "\n",
        "        if week_cols:\n",
        "            df_features[f'week_{week}_total'] = df_features[week_cols].sum(axis=1)\n",
        "            df_features[f'week_{week}_mean'] = df_features[week_cols].mean(axis=1)\n",
        "            df_features[f'week_{week}_std'] = df_features[week_cols].std(axis=1).fillna(0)\n",
        "            df_features[f'week_{week}_max'] = df_features[week_cols].max(axis=1)\n",
        "            df_features[f'week_{week}_min'] = df_features[week_cols].min(axis=1)\n",
        "            df_features[f'week_{week}_range'] = df_features[f'week_{week}_max'] - df_features[f'week_{week}_min']\n",
        "\n",
        "    # 2. ACTIVITY TYPE AGGREGATIONS\n",
        "    print(\"   - Creating activity type aggregations...\")\n",
        "    for activity in activity_types:\n",
        "        activity_cols_type = [col for col in activity_cols if activity in col]\n",
        "        if activity_cols_type:\n",
        "            df_features[f'total_{activity}'] = df_features[activity_cols_type].sum(axis=1)\n",
        "            df_features[f'mean_{activity}'] = df_features[activity_cols_type].mean(axis=1)\n",
        "            df_features[f'std_{activity}'] = df_features[activity_cols_type].std(axis=1).fillna(0)\n",
        "            df_features[f'max_{activity}'] = df_features[activity_cols_type].max(axis=1)\n",
        "\n",
        "            # Early activity (days 1-3)\n",
        "            early_cols = [col for col in activity_cols_type if any(f'day_{d}_' in col for d in range(1, 4))]\n",
        "            if early_cols:\n",
        "                df_features[f'early_{activity}'] = df_features[early_cols].sum(axis=1)\n",
        "\n",
        "            # Late activity (days 21-30)\n",
        "            late_cols = [col for col in activity_cols_type if any(f'day_{d}_' in col for d in range(21, 31))]\n",
        "            if late_cols:\n",
        "                df_features[f'late_{activity}'] = df_features[late_cols].sum(axis=1)\n",
        "\n",
        "    # 3. TEMPORAL PATTERNS\n",
        "    print(\"   - Creating temporal patterns...\")\n",
        "\n",
        "    # Early engagement (critical period)\n",
        "    early_days = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(1, 4))]\n",
        "    df_features['early_engagement'] = df_features[early_days].sum(axis=1)\n",
        "\n",
        "    # First week vs last week comparison\n",
        "    first_week = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(1, 8))]\n",
        "    last_week = [col for col in activity_cols if any(f'day_{d}_' in col for d in range(24, 31))]\n",
        "\n",
        "    df_features['first_week_total'] = df_features[first_week].sum(axis=1)\n",
        "    df_features['last_week_total'] = df_features[last_week].sum(axis=1)\n",
        "    df_features['engagement_decline'] = (df_features['first_week_total'] - df_features['last_week_total']) / (df_features['first_week_total'] + 1)\n",
        "\n",
        "    # Activity consistency (coefficient of variation)\n",
        "    for activity in activity_types:\n",
        "        activity_cols_type = [col for col in activity_cols if activity in col]\n",
        "        if activity_cols_type:\n",
        "            means = df_features[activity_cols_type].mean(axis=1)\n",
        "            stds = df_features[activity_cols_type].std(axis=1).fillna(0)\n",
        "            df_features[f'cv_{activity}'] = stds / (means + 1)  # Coefficient of variation\n",
        "\n",
        "    # 4. INTERACTION FEATURES\n",
        "    print(\"   - Creating interaction features...\")\n",
        "\n",
        "    # Problem-solving to video watching ratio\n",
        "    if 'total_problem' in df_features.columns and 'total_video' in df_features.columns:\n",
        "        df_features['problem_video_ratio'] = df_features['total_problem'] / (df_features['total_video'] + 1)\n",
        "\n",
        "    # Discussion to access ratio (social engagement)\n",
        "    if 'total_discussion' in df_features.columns and 'total_access' in df_features.columns:\n",
        "        df_features['discussion_access_ratio'] = df_features['total_discussion'] / (df_features['total_access'] + 1)\n",
        "\n",
        "    # Wiki usage efficiency\n",
        "    if 'total_wiki' in df_features.columns and 'total_access' in df_features.columns:\n",
        "        df_features['wiki_efficiency'] = df_features['total_wiki'] / (df_features['total_access'] + 1)\n",
        "\n",
        "    # 5. ROLLING STATISTICS\n",
        "    print(\"   - Creating rolling statistics...\")\n",
        "\n",
        "    # Create daily totals for rolling calculations\n",
        "    daily_totals = []\n",
        "    for day in range(1, 31):\n",
        "        day_cols = [col for col in activity_cols if f'day_{day}_' in col]\n",
        "        if day_cols:\n",
        "            daily_totals.append(df_features[day_cols].sum(axis=1))\n",
        "\n",
        "    if daily_totals:\n",
        "        daily_df = pd.concat(daily_totals, axis=1)\n",
        "        daily_df.columns = [f'day_{i+1}_total' for i in range(len(daily_totals))]\n",
        "\n",
        "        # Rolling averages (compute for each row)\n",
        "        rolling_3_means = []\n",
        "        rolling_7_means = []\n",
        "        rolling_3_trends = []\n",
        "\n",
        "        for idx in range(len(daily_df)):\n",
        "            row_values = daily_df.iloc[idx].values\n",
        "\n",
        "            # 3-day rolling mean\n",
        "            if len(row_values) >= 3:\n",
        "                rolling_3 = pd.Series(row_values).rolling(3).mean()\n",
        "                rolling_3_means.append(rolling_3.dropna().mean())\n",
        "            else:\n",
        "                rolling_3_means.append(0)\n",
        "\n",
        "            # 7-day rolling mean\n",
        "            if len(row_values) >= 7:\n",
        "                rolling_7 = pd.Series(row_values).rolling(7).mean()\n",
        "                rolling_7_means.append(rolling_7.dropna().mean())\n",
        "            else:\n",
        "                rolling_7_means.append(0)\n",
        "\n",
        "            # 3-day rolling trend (slope)\n",
        "            if len(row_values) >= 3:\n",
        "                try:\n",
        "                    trend_values = []\n",
        "                    for i in range(2, len(row_values)):\n",
        "                        window = row_values[i-2:i+1]\n",
        "                        if len(window) == 3:\n",
        "                            slope = np.polyfit(range(3), window, 1)[0]\n",
        "                            trend_values.append(slope)\n",
        "                    rolling_3_trends.append(np.mean(trend_values) if trend_values else 0)\n",
        "                except:\n",
        "                    rolling_3_trends.append(0)\n",
        "            else:\n",
        "                rolling_3_trends.append(0)\n",
        "\n",
        "        df_features['rolling_3_mean'] = rolling_3_means\n",
        "        df_features['rolling_7_mean'] = rolling_7_means\n",
        "        df_features['rolling_3_trend'] = rolling_3_trends\n",
        "\n",
        "    # 6. STATISTICAL FEATURES\n",
        "    print(\"   - Creating statistical features...\")\n",
        "\n",
        "    # Overall activity statistics\n",
        "    all_activity_cols = [col for col in activity_cols if 'day_' in col]\n",
        "    df_features['total_activity'] = df_features[all_activity_cols].sum(axis=1)\n",
        "    df_features['mean_daily_activity'] = df_features[all_activity_cols].mean(axis=1)\n",
        "    df_features['std_daily_activity'] = df_features[all_activity_cols].std(axis=1).fillna(0)\n",
        "    df_features['skew_activity'] = df_features[all_activity_cols].skew(axis=1).fillna(0)\n",
        "    df_features['kurtosis_activity'] = df_features[all_activity_cols].kurtosis(axis=1).fillna(0)\n",
        "\n",
        "    # Activity streaks (consecutive days with activity)\n",
        "    def calculate_max_streak(row):\n",
        "        \"\"\"Calculate maximum consecutive days with activity\"\"\"\n",
        "        try:\n",
        "            binary_activity = (row > 0).astype(int)\n",
        "            streaks = []\n",
        "            current_streak = 0\n",
        "\n",
        "            for value in binary_activity:\n",
        "                if value == 1:\n",
        "                    current_streak += 1\n",
        "                else:\n",
        "                    if current_streak > 0:\n",
        "                        streaks.append(current_streak)\n",
        "                    current_streak = 0\n",
        "\n",
        "            # Add final streak if it exists\n",
        "            if current_streak > 0:\n",
        "                streaks.append(current_streak)\n",
        "\n",
        "            return max(streaks) if streaks else 0\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    df_features['max_activity_streak'] = df_features[all_activity_cols].apply(\n",
        "        calculate_max_streak, axis=1\n",
        "    )\n",
        "\n",
        "    print(f\"   - Total features after engineering: {df_features.shape[1] - 1}\")\n",
        "    print(f\"   - Added {df_features.shape[1] - df.shape[1]} new features\")\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ADVANCED MODEL TRAINING WITH OPTIMIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_models_with_hyperparameter_optimization(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train multiple models with advanced hyperparameter optimization\"\"\"\n",
        "    print(\"\\nğŸ¤– Training models with hyperparameter optimization...\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Define sampling strategies\n",
        "    sampling_strategies = {\n",
        "        'SMOTE': SMOTE(random_state=RANDOM_STATE, k_neighbors=3),\n",
        "        'ADASYN': ADASYN(random_state=RANDOM_STATE),\n",
        "        'BorderlineSMOTE': BorderlineSMOTE(random_state=RANDOM_STATE),\n",
        "        'SMOTEENN': SMOTEENN(random_state=RANDOM_STATE),\n",
        "        'SMOTETomek': SMOTETomek(random_state=RANDOM_STATE)\n",
        "    }\n",
        "\n",
        "    # XGBoost with extensive hyperparameter tuning\n",
        "    print(\"\\n   ğŸ” Optimizing XGBoost...\")\n",
        "\n",
        "    xgb_param_dist = {\n",
        "        'classifier__n_estimators': randint(200, 2000),\n",
        "        'classifier__max_depth': randint(3, 15),\n",
        "        'classifier__learning_rate': loguniform(0.01, 0.3),\n",
        "        'classifier__subsample': uniform(0.6, 0.35),\n",
        "        'classifier__colsample_bytree': uniform(0.6, 0.35),\n",
        "        'classifier__colsample_bylevel': uniform(0.6, 0.35),\n",
        "        'classifier__min_child_weight': randint(1, 10),\n",
        "        'classifier__gamma': uniform(0, 1),\n",
        "        'classifier__reg_alpha': loguniform(1e-8, 10),\n",
        "        'classifier__reg_lambda': loguniform(1e-8, 10)\n",
        "    }\n",
        "\n",
        "    best_xgb_score = 0\n",
        "    best_xgb_model = None\n",
        "\n",
        "    for sampling_name, sampling in sampling_strategies.items():\n",
        "        print(f\"      Testing XGBoost + {sampling_name}...\")\n",
        "\n",
        "        pipeline = ImbPipeline([\n",
        "            ('sampling', sampling),\n",
        "            ('classifier', xgb.XGBClassifier(\n",
        "                random_state=RANDOM_STATE,\n",
        "                eval_metric='logloss',\n",
        "                tree_method='hist',\n",
        "                objective='binary:logistic'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            xgb_param_dist,\n",
        "            n_iter=100,  # Increased iterations\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if search.best_score_ > best_xgb_score:\n",
        "            best_xgb_score = search.best_score_\n",
        "            best_xgb_model = search.best_estimator_\n",
        "            best_xgb_sampling = sampling_name\n",
        "\n",
        "    # Evaluate best XGBoost\n",
        "    y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "    y_pred_proba_xgb = best_xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results['XGBoost'] = {\n",
        "        'model': best_xgb_model,\n",
        "        'sampling': best_xgb_sampling,\n",
        "        'cv_score': best_xgb_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_xgb),\n",
        "        'test_precision': precision_score(y_test, y_pred_xgb),\n",
        "        'test_recall': recall_score(y_test, y_pred_xgb),\n",
        "        'test_f1': f1_score(y_test, y_pred_xgb),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_xgb),\n",
        "        'predictions': y_pred_xgb,\n",
        "        'probabilities': y_pred_proba_xgb\n",
        "    }\n",
        "\n",
        "    print(f\"      Best XGBoost: {best_xgb_sampling} - CV AUC: {best_xgb_score:.4f}, Test AUC: {results['XGBoost']['test_auc']:.4f}\")\n",
        "\n",
        "    # LightGBM optimization\n",
        "    print(\"\\n   ğŸ” Optimizing LightGBM...\")\n",
        "\n",
        "    lgb_param_dist = {\n",
        "        'classifier__n_estimators': randint(200, 2000),\n",
        "        'classifier__max_depth': randint(3, 15),\n",
        "        'classifier__learning_rate': loguniform(0.01, 0.3),\n",
        "        'classifier__num_leaves': randint(20, 300),\n",
        "        'classifier__min_child_samples': randint(10, 200),\n",
        "        'classifier__min_child_weight': loguniform(1e-5, 10),\n",
        "        'classifier__subsample': uniform(0.6, 0.35),\n",
        "        'classifier__colsample_bytree': uniform(0.6, 0.35),\n",
        "        'classifier__reg_alpha': loguniform(1e-8, 10),\n",
        "        'classifier__reg_lambda': loguniform(1e-8, 10)\n",
        "    }\n",
        "\n",
        "    best_lgb_score = 0\n",
        "    best_lgb_model = None\n",
        "\n",
        "    for sampling_name, sampling in sampling_strategies.items():\n",
        "        pipeline = ImbPipeline([\n",
        "            ('sampling', sampling),\n",
        "            ('classifier', lgb.LGBMClassifier(\n",
        "                random_state=RANDOM_STATE,\n",
        "                verbose=-1,\n",
        "                objective='binary',\n",
        "                boosting_type='gbdt'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            lgb_param_dist,\n",
        "            n_iter=80,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if search.best_score_ > best_lgb_score:\n",
        "            best_lgb_score = search.best_score_\n",
        "            best_lgb_model = search.best_estimator_\n",
        "            best_lgb_sampling = sampling_name\n",
        "\n",
        "    # Evaluate best LightGBM\n",
        "    y_pred_lgb = best_lgb_model.predict(X_test)\n",
        "    y_pred_proba_lgb = best_lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results['LightGBM'] = {\n",
        "        'model': best_lgb_model,\n",
        "        'sampling': best_lgb_sampling,\n",
        "        'cv_score': best_lgb_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_lgb),\n",
        "        'test_precision': precision_score(y_test, y_pred_lgb),\n",
        "        'test_recall': recall_score(y_test, y_pred_lgb),\n",
        "        'test_f1': f1_score(y_test, y_pred_lgb),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_lgb),\n",
        "        'predictions': y_pred_lgb,\n",
        "        'probabilities': y_pred_proba_lgb\n",
        "    }\n",
        "\n",
        "    print(f\"      Best LightGBM: {best_lgb_sampling} - CV AUC: {best_lgb_score:.4f}, Test AUC: {results['LightGBM']['test_auc']:.4f}\")\n",
        "\n",
        "    # CatBoost optimization\n",
        "    print(\"\\n   ğŸ” Optimizing CatBoost...\")\n",
        "\n",
        "    catboost_param_dist = {\n",
        "        'classifier__iterations': randint(200, 1500),\n",
        "        'classifier__depth': randint(4, 12),\n",
        "        'classifier__learning_rate': loguniform(0.01, 0.3),\n",
        "        'classifier__l2_leaf_reg': loguniform(1, 30),\n",
        "        'classifier__border_count': randint(32, 255),\n",
        "        'classifier__bagging_temperature': uniform(0, 10)\n",
        "    }\n",
        "\n",
        "    best_cat_score = 0\n",
        "    best_cat_model = None\n",
        "\n",
        "    for sampling_name, sampling in list(sampling_strategies.items())[:3]:  # Test top 3 sampling methods\n",
        "        pipeline = ImbPipeline([\n",
        "            ('sampling', sampling),\n",
        "            ('classifier', CatBoostClassifier(\n",
        "                random_state=RANDOM_STATE,\n",
        "                verbose=False,\n",
        "                objective='Logloss',\n",
        "                eval_metric='AUC'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            catboost_param_dist,\n",
        "            n_iter=60,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if search.best_score_ > best_cat_score:\n",
        "            best_cat_score = search.best_score_\n",
        "            best_cat_model = search.best_estimator_\n",
        "            best_cat_sampling = sampling_name\n",
        "\n",
        "    # Evaluate best CatBoost\n",
        "    y_pred_cat = best_cat_model.predict(X_test)\n",
        "    y_pred_proba_cat = best_cat_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results['CatBoost'] = {\n",
        "        'model': best_cat_model,\n",
        "        'sampling': best_cat_sampling,\n",
        "        'cv_score': best_cat_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_cat),\n",
        "        'test_precision': precision_score(y_test, y_pred_cat),\n",
        "        'test_recall': recall_score(y_test, y_pred_cat),\n",
        "        'test_f1': f1_score(y_test, y_pred_cat),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_cat),\n",
        "        'predictions': y_pred_cat,\n",
        "        'probabilities': y_pred_proba_cat\n",
        "    }\n",
        "\n",
        "    print(f\"      Best CatBoost: {best_cat_sampling} - CV AUC: {best_cat_score:.4f}, Test AUC: {results['CatBoost']['test_auc']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ENSEMBLE MODEL CREATION\n",
        "# ============================================================================\n",
        "\n",
        "def create_ensemble_model(models_results, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Create ensemble model from best individual models\"\"\"\n",
        "    print(\"\\nğŸ¯ Creating ensemble model...\")\n",
        "\n",
        "    # Get top 3 models by AUC\n",
        "    sorted_models = sorted(models_results.items(), key=lambda x: x[1]['test_auc'], reverse=True)\n",
        "    top_models = sorted_models[:3]\n",
        "\n",
        "    print(\"   Top models for ensemble:\")\n",
        "    for name, result in top_models:\n",
        "        print(f\"      - {name}: AUC {result['test_auc']:.4f}\")\n",
        "\n",
        "    # Create voting ensemble\n",
        "    estimators = [(name, result['model']) for name, result in top_models]\n",
        "\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=estimators,\n",
        "        voting='soft'  # Use probability voting\n",
        "    )\n",
        "\n",
        "    # Train ensemble\n",
        "    ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    y_pred_ensemble = ensemble.predict(X_test)\n",
        "    y_pred_proba_ensemble = ensemble.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    ensemble_results = {\n",
        "        'model': ensemble,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_ensemble),\n",
        "        'test_precision': precision_score(y_test, y_pred_ensemble),\n",
        "        'test_recall': recall_score(y_test, y_pred_ensemble),\n",
        "        'test_f1': f1_score(y_test, y_pred_ensemble),\n",
        "        'test_auc': roc_auc_score(y_test, y_pred_proba_ensemble),\n",
        "        'predictions': y_pred_ensemble,\n",
        "        'probabilities': y_pred_proba_ensemble\n",
        "    }\n",
        "\n",
        "    print(f\"   Ensemble AUC: {ensemble_results['test_auc']:.4f}\")\n",
        "\n",
        "    return ensemble_results\n",
        "\n",
        "# ============================================================================\n",
        "# 5. EXPLAINABLE AI ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "def explain_best_model(best_model, X_train, X_test, y_train, feature_names):\n",
        "    \"\"\"Generate SHAP explanations for the best model\"\"\"\n",
        "    print(\"\\nğŸ” Generating SHAP explanations...\")\n",
        "\n",
        "    try:\n",
        "        # Get the classifier from the pipeline\n",
        "        if hasattr(best_model, 'named_steps'):\n",
        "            classifier = best_model.named_steps['classifier']\n",
        "            # Apply sampling to get training data for explainer\n",
        "            sampler = best_model.named_steps['sampling']\n",
        "            X_resampled, _ = sampler.fit_resample(X_train, y_train)\n",
        "        else:\n",
        "            classifier = best_model\n",
        "            X_resampled = X_train\n",
        "\n",
        "        # Create explainer based on model type\n",
        "        if isinstance(classifier, (xgb.XGBClassifier, lgb.LGBMClassifier)):\n",
        "            explainer = shap.TreeExplainer(classifier)\n",
        "        else:\n",
        "            # For CatBoost or other models, use a sample for background\n",
        "            explainer = shap.Explainer(classifier, X_resampled[:100])\n",
        "\n",
        "        # Calculate SHAP values for a sample of test data\n",
        "        sample_size = min(500, len(X_test))\n",
        "        X_sample = X_test[:sample_size]\n",
        "\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "        # Handle different SHAP output formats\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_values = shap_values[1]  # Binary classification positive class\n",
        "\n",
        "        # Calculate feature importance\n",
        "        feature_importance = np.abs(shap_values).mean(axis=0)\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': feature_importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"   Top 15 most important features:\")\n",
        "        print(importance_df.head(15).to_string(index=False))\n",
        "\n",
        "        return importance_df, shap_values\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error in SHAP analysis: {e}\")\n",
        "        # Fallback to permutation importance\n",
        "        from sklearn.inspection import permutation_importance\n",
        "\n",
        "        # Get the target values for the test set\n",
        "        y_test_values = y_train.iloc[:len(X_test)] if hasattr(y_train, 'iloc') else y_train[:len(X_test)]\n",
        "\n",
        "        perm_importance = permutation_importance(\n",
        "            best_model, X_test, y_test_values,\n",
        "            n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1\n",
        "        )\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': perm_importance.importances_mean\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(\"   Top 15 most important features (permutation importance):\")\n",
        "        print(importance_df.head(15).to_string(index=False))\n",
        "\n",
        "        return importance_df, None\n",
        "\n",
        "# ============================================================================\n",
        "# 6. RESULTS ANALYSIS AND REPORTING\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_and_report_results(models_results, ensemble_results, feature_importance):\n",
        "    \"\"\"Generate comprehensive results analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“Š FINAL RESULTS ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = models_results.copy()\n",
        "    all_results['Ensemble'] = ensemble_results\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Model': list(all_results.keys()),\n",
        "        'Accuracy': [result['test_accuracy'] for result in all_results.values()],\n",
        "        'Precision': [result['test_precision'] for result in all_results.values()],\n",
        "        'Recall': [result['test_recall'] for result in all_results.values()],\n",
        "        'F1-Score': [result['test_f1'] for result in all_results.values()],\n",
        "        'AUC-ROC': [result['test_auc'] for result in all_results.values()]\n",
        "    })\n",
        "\n",
        "    results_df = results_df.round(4)\n",
        "    results_df = results_df.sort_values('AUC-ROC', ascending=False)\n",
        "\n",
        "    print(\"\\nğŸ† MODEL PERFORMANCE COMPARISON:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Best model\n",
        "    best_model_name = results_df.iloc[0]['Model']\n",
        "    best_auc = results_df.iloc[0]['AUC-ROC']\n",
        "    best_accuracy = results_df.iloc[0]['Accuracy']\n",
        "\n",
        "    print(f\"\\nğŸ¥‡ BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   - AUC-ROC: {best_auc:.4f}\")\n",
        "    print(f\"   - Accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"   - Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
        "    print(f\"   - Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
        "    print(f\"   - F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "\n",
        "    # Feature importance insights\n",
        "    print(f\"\\nğŸ” KEY INSIGHTS FROM FEATURE IMPORTANCE:\")\n",
        "\n",
        "    # Categorize features\n",
        "    early_features = feature_importance[feature_importance['feature'].str.contains('day_[1-3]|early_')].head(5)\n",
        "    activity_features = feature_importance[feature_importance['feature'].str.contains('total_|mean_')].head(5)\n",
        "    temporal_features = feature_importance[feature_importance['feature'].str.contains('week_|decline|trend')].head(5)\n",
        "\n",
        "    if not early_features.empty:\n",
        "        print(f\"   ğŸš¨ Early Warning Indicators:\")\n",
        "        for _, row in early_features.iterrows():\n",
        "            print(f\"      - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "    if not activity_features.empty:\n",
        "        print(f\"   ğŸ“Š Activity Pattern Indicators:\")\n",
        "        for _, row in activity_features.iterrows():\n",
        "            print(f\"      - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "    if not temporal_features.empty:\n",
        "        print(f\"   ğŸ“ˆ Temporal Pattern Indicators:\")\n",
        "        for _, row in temporal_features.iterrows():\n",
        "            print(f\"      - {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "    # Performance improvements\n",
        "    baseline_auc = min(results_df['AUC-ROC'])\n",
        "    improvement = ((best_auc - baseline_auc) / baseline_auc) * 100\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ PERFORMANCE IMPROVEMENTS:\")\n",
        "    print(f\"   - Best model improvement over baseline: {improvement:.2f}%\")\n",
        "    print(f\"   - Achieved target of >90% AUC: {'âœ… YES' if best_auc > 0.90 else 'âŒ NO'}\")\n",
        "    print(f\"   - Model ready for production: {'âœ… YES' if best_auc > 0.85 else 'âŒ NO'}\")\n",
        "\n",
        "    return results_df, best_model_name\n",
        "\n",
        "# ============================================================================\n",
        "# 7. MAIN EXECUTION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"ğŸ¯ Student Dropout Prediction - Best Accuracy Pipeline\")\n",
        "    print(\"Optimized for maximum performance with advanced ML techniques\")\n",
        "\n",
        "    # 1. Load and preprocess data\n",
        "    df = load_and_preprocess_data('model1_210_features.csv')\n",
        "\n",
        "    # 2. Advanced feature engineering\n",
        "    df_features = create_advanced_features(df)\n",
        "\n",
        "    # 3. Prepare data for modeling\n",
        "    print(\"\\nâš™ï¸ Preparing data for modeling...\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df_features.drop('dropout', axis=1)\n",
        "    y = df_features['dropout']\n",
        "\n",
        "    print(f\"Final feature count: {X.shape[1]}\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE,\n",
        "        stratify=y, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = RobustScaler()  # More robust to outliers\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "    # 4. Train models with hyperparameter optimization\n",
        "    models_results = train_models_with_hyperparameter_optimization(\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # 5. Create ensemble model\n",
        "    ensemble_results = create_ensemble_model(\n",
        "        models_results, X_train_scaled, X_test_scaled, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # 6. Determine best model\n",
        "    all_results = models_results.copy()\n",
        "    all_results['Ensemble'] = ensemble_results\n",
        "\n",
        "    best_model_name = max(all_results.keys(), key=lambda k: all_results[k]['test_auc'])\n",
        "    best_model = all_results[best_model_name]['model']\n",
        "\n",
        "    print(f\"\\nğŸ† Best performing model: {best_model_name}\")\n",
        "    print(f\"Best AUC: {all_results[best_model_name]['test_auc']:.4f}\")\n",
        "\n",
        "    # 7. Explainable AI analysis\n",
        "    feature_importance, shap_values = explain_best_model(\n",
        "        best_model, X_train_scaled, X_test_scaled, y_train, X.columns.tolist()\n",
        "    )\n",
        "\n",
        "    # 8. Final analysis and reporting\n",
        "    results_df, final_best_model = analyze_and_report_results(\n",
        "        models_results, ensemble_results, feature_importance\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'best_model': best_model,\n",
        "        'best_model_name': final_best_model,\n",
        "        'feature_importance': feature_importance,\n",
        "        'scaler': scaler,\n",
        "        'models_results': all_results\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# 8. EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    pipeline_results = main()\n",
        "\n",
        "    # Optional: Save results\n",
        "    try:\n",
        "        pipeline_results['results_df'].to_csv('dropout_prediction_results.csv', index=False)\n",
        "        pipeline_results['feature_importance'].to_csv('feature_importance.csv', index=False)\n",
        "        print(\"\\nğŸ’¾ Results saved to CSV files\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸ Could not save results: {e}\")\n",
        "\n",
        "    print(f\"\\nğŸš€ Best model achieved AUC: {pipeline_results['results_df'].iloc[0]['AUC-ROC']:.4f}\")\n",
        "    print(\"Pipeline ready for production deployment! ğŸ¯\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMou5szI+qWJovGVitYZBDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}